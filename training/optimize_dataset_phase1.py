#!/usr/bin/env python3
"""
ğŸ”§ Phase 1: æ•¸æ“šé›†å„ªåŒ–è…³æœ¬
è§£æ±º YOLO12 è¨“ç·´çš„æ ¸å¿ƒå•é¡Œï¼š
1. é¡åˆ¥ç¼ºå¤±ï¼ˆstem_down, slurï¼‰
2. é¡åˆ¥ä¸å¹³è¡¡
3. é©—è­‰é›†éå°
"""

import json
import shutil
import random
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple
import os

# ============== é…ç½® ==============
ORIGINAL_DATASET = Path("datasets/yolo_harmony_v2_35classes")
OPTIMIZED_DATASET = Path("datasets/yolo_harmony_v2_optimized")
RANDOM_SEED = 42

# å•é¡Œé¡åˆ¥è™•ç†ç­–ç•¥
PROBLEM_CLASSES = {
    3: "merge_to_2",   # stem_down â†’ stem_up (ç„¡æ³•å€åˆ†)
    30: "exclude",     # slur (ç„¡æ•¸æ“šï¼Œæš«æ™‚æ’é™¤)
}

# ç›®æ¨™é¡åˆ¥æ•¸é‡ï¼ˆæ’é™¤å•é¡Œé¡åˆ¥å¾Œï¼‰
FINAL_NUM_CLASSES = 33  # 35 - 2

# æ–°çš„é¡åˆ¥æ˜ å°„ï¼ˆæ’é™¤ 3 å’Œ 30 å¾Œé‡æ–°ç·¨è™Ÿï¼‰
# åŸå§‹: 0,1,2,3,4,5,...,30,...,34
# æ–°çš„: 0,1,2,skip,3,4,...,skip,...,32
OLD_TO_NEW_CLASS = {}
new_id = 0
for old_id in range(35):
    if old_id == 3:
        OLD_TO_NEW_CLASS[3] = 2  # stem_down â†’ stem_up
    elif old_id == 30:
        OLD_TO_NEW_CLASS[30] = -1  # æ’é™¤
    else:
        OLD_TO_NEW_CLASS[old_id] = new_id
        new_id += 1

# ============== å‡½æ•¸ ==============

def load_labels(label_path: Path) -> List[Tuple[int, float, float, float, float]]:
    """è®€å– YOLO æ ¼å¼æ¨™è¨»"""
    labels = []
    if label_path.exists():
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    cls = int(parts[0])
                    x, y, w, h = map(float, parts[1:5])
                    labels.append((cls, x, y, w, h))
    return labels

def save_labels(label_path: Path, labels: List[Tuple[int, float, float, float, float]]):
    """ä¿å­˜ YOLO æ ¼å¼æ¨™è¨»"""
    label_path.parent.mkdir(parents=True, exist_ok=True)
    with open(label_path, 'w') as f:
        for cls, x, y, w, h in labels:
            f.write(f"{cls} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\n")

def remap_labels(labels: List[Tuple], old_to_new: Dict[int, int]) -> List[Tuple]:
    """é‡æ–°æ˜ å°„é¡åˆ¥ ID"""
    remapped = []
    for cls, x, y, w, h in labels:
        new_cls = old_to_new.get(cls, -1)
        if new_cls >= 0:  # æ’é™¤ -1 (è¢«ç§»é™¤çš„é¡åˆ¥)
            remapped.append((new_cls, x, y, w, h))
    return remapped

def analyze_dataset(dataset_path: Path) -> Dict:
    """åˆ†ææ•¸æ“šé›†çµ±è¨ˆ"""
    stats = {
        'train': {'images': 0, 'labels': Counter()},
        'val': {'images': 0, 'labels': Counter()}
    }

    for split in ['train', 'val']:
        label_dir = dataset_path / split / 'labels'
        if label_dir.exists():
            for label_file in label_dir.glob('*.txt'):
                stats[split]['images'] += 1
                labels = load_labels(label_file)
                for cls, *_ in labels:
                    stats[split]['labels'][cls] += 1

    return stats

def create_optimized_dataset():
    """å‰µå»ºå„ªåŒ–å¾Œçš„æ•¸æ“šé›†"""
    print("=" * 60)
    print("ğŸ”§ Phase 1: æ•¸æ“šé›†å„ªåŒ–")
    print("=" * 60)

    # 1. åˆ†æåŸå§‹æ•¸æ“šé›†
    print("\nğŸ“Š åˆ†æåŸå§‹æ•¸æ“šé›†...")
    original_stats = analyze_dataset(ORIGINAL_DATASET)

    print(f"\nåŸå§‹æ•¸æ“šé›†çµ±è¨ˆï¼š")
    print(f"  è¨“ç·´é›†: {original_stats['train']['images']} å¼µåœ–ç‰‡")
    print(f"  é©—è­‰é›†: {original_stats['val']['images']} å¼µåœ–ç‰‡")

    # é¡¯ç¤ºå•é¡Œé¡åˆ¥
    print(f"\nğŸ”´ å•é¡Œé¡åˆ¥åˆ†æï¼š")
    for cls, action in PROBLEM_CLASSES.items():
        train_count = original_stats['train']['labels'].get(cls, 0)
        val_count = original_stats['val']['labels'].get(cls, 0)
        print(f"  Class {cls}: train={train_count}, val={val_count} â†’ {action}")

    # 2. å‰µå»ºå„ªåŒ–æ•¸æ“šé›†ç›®éŒ„
    print(f"\nğŸ“ å‰µå»ºå„ªåŒ–æ•¸æ“šé›†ç›®éŒ„: {OPTIMIZED_DATASET}")
    if OPTIMIZED_DATASET.exists():
        shutil.rmtree(OPTIMIZED_DATASET)

    for split in ['train', 'val']:
        (OPTIMIZED_DATASET / split / 'images').mkdir(parents=True)
        (OPTIMIZED_DATASET / split / 'labels').mkdir(parents=True)

    # 3. æ”¶é›†æ‰€æœ‰åœ–ç‰‡ä¸¦é‡æ–°åˆ†é…
    print("\nğŸ”„ é‡æ–°åˆ†é…è¨“ç·´/é©—è­‰é›†...")
    all_samples = []

    for split in ['train', 'val']:
        img_dir = ORIGINAL_DATASET / split / 'images'
        label_dir = ORIGINAL_DATASET / split / 'labels'

        if img_dir.exists():
            for img_file in img_dir.glob('*.png'):
                label_file = label_dir / img_file.with_suffix('.txt').name
                if label_file.exists():
                    all_samples.append({
                        'img': img_file,
                        'label': label_file
                    })

    print(f"  ç¸½å…±æ”¶é›†: {len(all_samples)} å€‹æ¨£æœ¬")

    # 4. éš¨æ©Ÿæ‰“äº‚ä¸¦é‡æ–°åˆ†é… (80:20)
    random.seed(RANDOM_SEED)
    random.shuffle(all_samples)

    split_idx = int(len(all_samples) * 0.8)
    train_samples = all_samples[:split_idx]
    val_samples = all_samples[split_idx:]

    print(f"  æ–°è¨“ç·´é›†: {len(train_samples)} å¼µ (80%)")
    print(f"  æ–°é©—è­‰é›†: {len(val_samples)} å¼µ (20%)")

    # 5. è™•ç†ä¸¦ä¿å­˜æ•¸æ“š
    print("\nğŸ”§ è™•ç†æ¨™è¨»ä¸¦ä¿å­˜...")

    new_stats = {
        'train': {'images': 0, 'labels': Counter()},
        'val': {'images': 0, 'labels': Counter()}
    }

    for split, samples in [('train', train_samples), ('val', val_samples)]:
        for sample in samples:
            # è®€å–åŸå§‹æ¨™è¨»
            labels = load_labels(sample['label'])

            # é‡æ–°æ˜ å°„é¡åˆ¥
            remapped = remap_labels(labels, OLD_TO_NEW_CLASS)

            if remapped:  # åªä¿å­˜æœ‰æ¨™è¨»çš„åœ–ç‰‡
                # è¤‡è£½åœ–ç‰‡
                dst_img = OPTIMIZED_DATASET / split / 'images' / sample['img'].name
                shutil.copy2(sample['img'], dst_img)

                # ä¿å­˜é‡æ–°æ˜ å°„çš„æ¨™è¨»
                dst_label = OPTIMIZED_DATASET / split / 'labels' / sample['label'].name
                save_labels(dst_label, remapped)

                new_stats[split]['images'] += 1
                for cls, *_ in remapped:
                    new_stats[split]['labels'][cls] += 1

    # 6. ç”Ÿæˆæ–°çš„ YAML é…ç½®
    print("\nğŸ“ ç”Ÿæˆæ–°çš„æ•¸æ“šé›†é…ç½®...")

    # æ–°çš„é¡åˆ¥åç¨±ï¼ˆæ’é™¤ stem_down å’Œ slurï¼‰
    original_names = [
        "notehead_filled", "notehead_hollow", "stem", "beam",  # 0-3 (åŸ4è®Šæ–°3)
        "flag_8th", "flag_16th", "flag_32nd", "augmentation_dot", "tie",  # 4-8
        "clef_treble", "clef_bass", "clef_alto", "clef_tenor",  # 9-12
        "accidental_sharp", "accidental_flat", "accidental_natural",  # 13-15
        "accidental_double_sharp", "accidental_double_flat",  # 16-17
        "rest_whole", "rest_half", "rest_quarter", "rest_8th", "rest_16th",  # 18-22
        "barline", "barline_double", "barline_final", "barline_repeat",  # 23-26
        "time_signature", "key_signature",  # 27-28
        "fermata", "dynamic_soft", "dynamic_loud",  # 29-31 (åŸ31,32,33)
        "ledger_line"  # 32 (åŸ34)
    ]

    yaml_content = f"""# ğŸ”§ YOLO12 Harmony OMR V2 - Optimized (33 Classes)
# å„ªåŒ–ç‰ˆæœ¬ï¼šä¿®å¾©é¡åˆ¥ç¼ºå¤±ï¼Œé‡æ–°å¹³è¡¡æ•¸æ“šé›†
# Generated: 2025-11-22

path: {OPTIMIZED_DATASET.absolute()}
train: train/images
val: val/images

nc: {FINAL_NUM_CLASSES}

# é¡åˆ¥èªªæ˜ï¼š
# - stem_down (åŸ Class 3) å·²åˆä½µåˆ° stem (Class 2)
# - slur (åŸ Class 30) å·²æ’é™¤ï¼ˆç„¡æ•¸æ“šï¼‰

names:
"""
    for i, name in enumerate(original_names):
        yaml_content += f"  {i}: {name}\n"

    yaml_path = OPTIMIZED_DATASET / 'harmony_optimized.yaml'
    with open(yaml_path, 'w') as f:
        f.write(yaml_content)

    # 7. è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 60)
    print("âœ… å„ªåŒ–å®Œæˆï¼")
    print("=" * 60)

    print(f"\nğŸ“Š æ–°æ•¸æ“šé›†çµ±è¨ˆï¼š")
    print(f"  è¨“ç·´é›†: {new_stats['train']['images']} å¼µåœ–ç‰‡")
    print(f"  é©—è­‰é›†: {new_stats['val']['images']} å¼µåœ–ç‰‡")
    print(f"  é¡åˆ¥æ•¸: {FINAL_NUM_CLASSES}")

    print(f"\nğŸ”§ ä¸»è¦æ”¹é€²ï¼š")
    print(f"  1. stem_down åˆä½µåˆ° stemï¼ˆè§£æ±ºç„¡æ•¸æ“šå•é¡Œï¼‰")
    print(f"  2. slur æš«æ™‚æ’é™¤ï¼ˆè§£æ±ºç„¡æ•¸æ“šå•é¡Œï¼‰")
    print(f"  3. é©—è­‰é›†å¾ {original_stats['val']['images']} å¼µå¢åŠ åˆ° {new_stats['val']['images']} å¼µ")

    # é¡åˆ¥åˆ†å¸ƒ
    print(f"\nğŸ“ˆ æ–°é¡åˆ¥åˆ†å¸ƒï¼ˆå‰ 10 å€‹ï¼‰ï¼š")
    all_labels = new_stats['train']['labels'] + new_stats['val']['labels']
    for cls, count in all_labels.most_common(10):
        print(f"  Class {cls}: {count:,} å€‹æ¨™è¨»")

    # ç¨€æœ‰é¡åˆ¥è­¦å‘Š
    print(f"\nâš ï¸  ç¨€æœ‰é¡åˆ¥ï¼ˆ< 500 å€‹æ¨™è¨»ï¼‰ï¼š")
    for cls in range(FINAL_NUM_CLASSES):
        count = all_labels.get(cls, 0)
        if count < 500:
            print(f"  Class {cls} ({original_names[cls] if cls < len(original_names) else 'unknown'}): {count} å€‹")

    print(f"\nğŸ“ è¼¸å‡ºä½ç½®: {OPTIMIZED_DATASET.absolute()}")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {yaml_path.absolute()}")

    return new_stats

if __name__ == '__main__':
    os.chdir(Path(__file__).parent)
    create_optimized_dataset()

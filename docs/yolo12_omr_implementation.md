# YOLO12 ç«¯å´ OMR å®Œæ•´å¯¦ä½œè¦åŠƒ

**æ–‡ä»¶ç‰ˆæœ¬**: 1.0
**å»ºç«‹æ—¥æœŸ**: 2025-11-20
**ä½œè€…**: Claude + thc1006
**ç›®æ¨™**: åœ¨ Android æ‰‹æ©Ÿä¸Šå®Œå…¨ç«¯å´é‹è¡Œæ¨‚è­œè¾¨è­˜ï¼ˆæ”¯æ´æ‰€æœ‰ç­‰ç´šè£ç½®ï¼‰

---

## ğŸ“‹ ç›®éŒ„

1. [åŸ·è¡Œæ‘˜è¦](#åŸ·è¡Œæ‘˜è¦)
2. [æŠ€è¡“æ¶æ§‹](#æŠ€è¡“æ¶æ§‹)
3. [è³‡æ–™é›†æº–å‚™](#è³‡æ–™é›†æº–å‚™)
4. [æ¨¡å‹è¨“ç·´](#æ¨¡å‹è¨“ç·´)
5. [æ¨¡å‹é‡åŒ–èˆ‡åŒ¯å‡º](#æ¨¡å‹é‡åŒ–èˆ‡åŒ¯å‡º)
6. [Android æ•´åˆ](#android-æ•´åˆ)
7. [ç¬¦è™Ÿçµ„è£é‚è¼¯](#ç¬¦è™Ÿçµ„è£é‚è¼¯)
8. [å¤šè£ç½®é©é…ç­–ç•¥](#å¤šè£ç½®é©é…ç­–ç•¥)
9. [æ¸¬è©¦èˆ‡é©—è­‰](#æ¸¬è©¦èˆ‡é©—è­‰)
10. [é¢¨éšªèˆ‡ç·©è§£](#é¢¨éšªèˆ‡ç·©è§£)

---

## åŸ·è¡Œæ‘˜è¦

### æ ¸å¿ƒæ±ºç­–

- **æ¨¡å‹é¸æ“‡**: YOLO12s (ä¸») + YOLO12n (å‚™æ´)
- **è¨“ç·´ç¡¬é«”**: RTX 5060 GPU
- **ç›®æ¨™è£ç½®**: 2025 å¹´æ‰€æœ‰ç­‰ç´š Android æ‰‹æ©Ÿï¼ˆæœ€ä½ Android 8.0, 4GB RAMï¼‰
- **éƒ¨ç½²æ¡†æ¶**: TensorFlow Lite INT8 é‡åŒ–
- **å®Œå…¨é›¢ç·š**: ç„¡é›²ç«¯ä¾è³´ï¼Œæ‰€æœ‰é‹ç®—åœ¨æ‰‹æ©Ÿç«¯å®Œæˆ

### é æœŸæ•ˆèƒ½æŒ‡æ¨™

| è£ç½®ç­‰ç´š | è™•ç†å™¨ | æ¨¡å‹ | æ¨è«–æ™‚é–“ | ç¸½æ™‚é–“ |
|---------|--------|------|----------|--------|
| ä½éš | SD 6 Gen 1 | YOLO12n INT8 | 0.6-1.3ç§’ | 2-3ç§’ |
| ä¸­éš | SD 7 Gen 3 | YOLO12s INT8 | 0.6-1.2ç§’ | 1.5-2.5ç§’ |
| æº–æ——è‰¦ | SD 7+ Gen 3 | YOLO12s INT8 | 0.4-0.7ç§’ | 1-2ç§’ |

**ç›®æ¨™æº–ç¢ºåº¦**: mAP@0.5 > 85%ï¼ˆéŸ³ç¬¦æª¢æ¸¬ï¼‰

### é–‹ç™¼æ™‚ç¨‹

- **Week 1**: è³‡æ–™æº–å‚™ + æ¨¡å‹è¨“ç·´
- **Week 2**: æ¨¡å‹åŒ¯å‡º + Android åŸºç¤æ•´åˆ
- **Week 3**: ç¬¦è™Ÿçµ„è£ + UI ä¸²æ¥
- **Week 4+**: å¤šè£ç½®æ¸¬è©¦ + å„ªåŒ–

---

## æŠ€è¡“æ¶æ§‹

### ç³»çµ±æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Android Application                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  UI Layer (Jetpack Compose)                                 â”‚
â”‚  â”œâ”€ CameraScreen: æ‹ç…§/é¸åœ–                                  â”‚
â”‚  â”œâ”€ ProcessingScreen: è¾¨è­˜é€²åº¦                               â”‚
â”‚  â””â”€ ResultScreen: éŒ¯èª¤æ¨™è¨˜ + èªªæ˜                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ViewModel Layer                                             â”‚
â”‚  â””â”€ OmrViewModel: å”èª¿ OMR + è¦å‰‡å¼•æ“                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Domain Layer                                                â”‚
â”‚  â”œâ”€ OmrClient (interface)                                    â”‚
â”‚  â”‚   â””â”€ Yolo12OmrClient (impl) â—„â”€â”€ **æœ¬æ–‡ä»¶é‡é»**            â”‚
â”‚  â”œâ”€ SymbolAssembler â—„â”€â”€ **æœ¬æ–‡ä»¶é‡é»**                       â”‚
â”‚  â””â”€ HarmonyRuleEngine (å·²å®Œæˆ)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Infrastructure Layer                                        â”‚
â”‚  â”œâ”€ TensorFlow Lite Interpreter                             â”‚
â”‚  â”‚   â”œâ”€ Model: yolo12s_int8.tflite (10MB)                   â”‚
â”‚  â”‚   â”œâ”€ Model: yolo12n_int8.tflite (3MB)                    â”‚
â”‚  â”‚   â””â”€ Delegates: NNAPI, GPU, Hexagon                      â”‚
â”‚  â””â”€ Image Processing (Bitmap â†’ Tensor)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¨“ç·´ç«¯ï¼ˆPC with RTX 5060ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Pipeline                                           â”‚
â”‚  â”œâ”€ MUSCIMA++ Dataset (91K symbols)                         â”‚
â”‚  â”œâ”€ DeepScoresV2 Dataset (151K symbols)                     â”‚
â”‚  â”œâ”€ YOLO12 Training (PyTorch)                               â”‚
â”‚  â”œâ”€ Model Export (ONNX â†’ TFLite)                            â”‚
â”‚  â””â”€ INT8 Quantization (4x compression)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è³‡æ–™æµè©³è§£

```
1. Input: æ¨‚è­œç…§ç‰‡ (Bitmap, ~2-4MP)
   â†“
2. Preprocessing:
   - Resize to 640x640
   - RGB â†’ float32 [0-1]
   - Normalize: (pixel - mean) / std
   â†“
3. YOLO12 Inference:
   - Input: float32[1, 640, 640, 3]
   - Output: float32[1, 8400, 84]
     - 8400 = num_anchors
     - 84 = 4 (bbox) + 80 (classes, æˆ‘å€‘åªç”¨ 20)
   â†“
4. NMS (Non-Maximum Suppression):
   - Confidence threshold: 0.25
   - IOU threshold: 0.45
   - Output: List<Detection> (~50-200 objects)
   â†“
5. Symbol Assembly:
   - Sort by Y (top to bottom) â†’ äº”ç·šè­œåˆ†çµ„
   - Sort by X (left to right) â†’ æ™‚é–“é †åº
   - Match noteheads + stems + accidentals
   - Generate ChordSnapshot list
   â†“
6. Harmony Analysis:
   - HarmonyRuleEngine.analyze(chords, keySignature)
   - Output: List<HarmonyIssue>
   â†“
7. UI Render:
   - Overlay bounding boxes on original image
   - Display Chinese error messages
```

---

## è³‡æ–™é›†æº–å‚™

### æ¨è–¦è³‡æ–™é›†

#### 1. MUSCIMA++ (å„ªå…ˆä½¿ç”¨)

```
ä¾†æº: https://github.com/OMR-Research/muscima-pp
è¦æ¨¡: 140 é æ¨‚è­œï¼Œ91,255 å€‹æ¨™è¨»ç¬¦è™Ÿ
æ ¼å¼: XML annotations + PNG images
æˆæ¬Š: CC BY-NC-SA 4.0

ç¬¦è™Ÿé¡åˆ¥ï¼ˆé©åˆå››éƒ¨å’Œè²ï¼‰:
- noteheadFull (å¯¦å¿ƒç¬¦é ­)
- noteheadHalf (ç©ºå¿ƒç¬¦é ­)
- stem (ç¬¦å¹¹)
- beam (é€£éŸ³ç·š)
- gClef, fClef (é«˜éŸ³/ä½éŸ³è­œè™Ÿ)
- accidentalSharp, accidentalFlat, accidentalNatural
- timeSignature-*, keySignature-*
- barline, measureSeparator
```

#### 2. DeepScoresV2 (è£œå……è³‡æ–™)

```
ä¾†æº: https://zenodo.org/record/4012193
è¦æ¨¡: 151,286 å€‹ç¬¦è™Ÿæ¨™è¨»
æ ¼å¼: COCO JSON format
å„ªå‹¢: åˆæˆè³‡æ–™ï¼Œé¡åˆ¥å¤šæ¨£
```

### è³‡æ–™è½‰æ›æµç¨‹

#### Step 1: ä¸‹è¼‰è³‡æ–™é›†

```bash
# å»ºç«‹è³‡æ–™ç›®éŒ„
mkdir -p training/datasets

cd training/datasets

# ä¸‹è¼‰ MUSCIMA++
git clone https://github.com/OMR-Research/muscima-pp.git

# ä¸‹è¼‰ DeepScoresV2
# (éœ€æ‰‹å‹•å¾ Zenodo ä¸‹è¼‰ï¼Œç´„ 2GB)
```

#### Step 2: è½‰æ›ç‚º YOLO æ ¼å¼

å»ºç«‹ `training/convert_dataset.py`:

```python
"""
å°‡ MUSCIMA++ XML æ¨™è¨»è½‰æ›ç‚º YOLO txt æ ¼å¼
"""
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Tuple

# å®šç¾©æˆ‘å€‘éœ€è¦çš„ 20 å€‹é¡åˆ¥ï¼ˆé‡å°å››éƒ¨å’Œè²ï¼‰
HARMONY_CLASSES = [
    # 0-5: éŸ³ç¬¦ç¬¦è™Ÿ
    "notehead_filled",
    "notehead_hollow",
    "stem_up",
    "stem_down",
    "beam",
    "flag",

    # 6-9: è­œè™Ÿ
    "clef_treble",
    "clef_bass",
    "clef_alto",
    "clef_tenor",

    # 10-12: è®ŠéŸ³è¨˜è™Ÿ
    "accidental_sharp",
    "accidental_flat",
    "accidental_natural",

    # 13-15: ç¯€å¥/å°ç¯€
    "rest_quarter",
    "rest_half",
    "rest_whole",
    "barline",

    # 16-19: èª¿è™Ÿæ‹è™Ÿ
    "time_signature",
    "key_signature",
    "staffline"
]

CLASS_TO_IDX = {c: i for i, c in enumerate(HARMONY_CLASSES)}


def parse_muscima_xml(xml_path: Path) -> List[Tuple[str, List[int]]]:
    """
    è§£æ MUSCIMA++ XML æª”æ¡ˆ
    è¿”å›: [(class_name, [x, y, width, height]), ...]
    """
    tree = ET.parse(xml_path)
    root = tree.getroot()

    annotations = []
    for node in root.findall('.//Node'):
        class_name = node.find('ClassName').text

        # æ˜ å°„ MUSCIMA é¡åˆ¥åˆ°æˆ‘å€‘çš„é¡åˆ¥
        mapped_class = map_muscima_class(class_name)
        if mapped_class is None:
            continue

        # å–å¾— bounding box
        top = int(node.find('Top').text)
        left = int(node.find('Left').text)
        width = int(node.find('Width').text)
        height = int(node.find('Height').text)

        annotations.append((mapped_class, [left, top, width, height]))

    return annotations


def map_muscima_class(muscima_class: str) -> str | None:
    """å°‡ MUSCIMA++ é¡åˆ¥æ˜ å°„åˆ°æˆ‘å€‘çš„é¡åˆ¥"""
    mapping = {
        'noteheadFull': 'notehead_filled',
        'noteheadHalf': 'notehead_hollow',
        'noteheadWhole': 'notehead_hollow',
        'stem': 'stem_up',  # å¾ŒçºŒå†åˆ¤æ–·æ–¹å‘
        'beam': 'beam',
        'g-clef': 'clef_treble',
        'f-clef': 'clef_bass',
        'c-clef': 'clef_alto',
        'sharp': 'accidental_sharp',
        'flat': 'accidental_flat',
        'natural': 'accidental_natural',
        'rest-quarter': 'rest_quarter',
        'rest-half': 'rest_half',
        'rest-whole': 'rest_whole',
        'barline': 'barline',
        'timeSignature': 'time_signature',
        'keySignature': 'key_signature',
        'staffLine': 'staffline',
    }

    return mapping.get(muscima_class)


def convert_to_yolo_format(
    annotations: List[Tuple[str, List[int]]],
    img_width: int,
    img_height: int
) -> List[str]:
    """
    è½‰æ›ç‚º YOLO æ ¼å¼
    æ ¼å¼: <class_id> <x_center> <y_center> <width> <height>
    æ‰€æœ‰å€¼æ­£è¦åŒ–åˆ° [0, 1]
    """
    yolo_lines = []

    for class_name, (x, y, w, h) in annotations:
        class_id = CLASS_TO_IDX[class_name]

        # è½‰æ›ç‚ºä¸­å¿ƒé»åº§æ¨™ä¸¦æ­£è¦åŒ–
        x_center = (x + w / 2) / img_width
        y_center = (y + h / 2) / img_height
        norm_width = w / img_width
        norm_height = h / img_height

        yolo_line = f"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}"
        yolo_lines.append(yolo_line)

    return yolo_lines


def main():
    """ä¸»è½‰æ›æµç¨‹"""
    muscima_dir = Path('datasets/muscima-pp')
    output_dir = Path('datasets/yolo_harmony')

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    (output_dir / 'images' / 'train').mkdir(parents=True, exist_ok=True)
    (output_dir / 'images' / 'val').mkdir(parents=True, exist_ok=True)
    (output_dir / 'labels' / 'train').mkdir(parents=True, exist_ok=True)
    (output_dir / 'labels' / 'val').mkdir(parents=True, exist_ok=True)

    # è™•ç†æ‰€æœ‰æ¨£æœ¬...
    # (å¯¦ä½œçœç•¥ï¼Œéœ€éæ­·æ‰€æœ‰ XML + åœ–ç‰‡)

    print(f"è½‰æ›å®Œæˆï¼")
    print(f"è¨“ç·´é›†: {len(train_images)} å¼µ")
    print(f"é©—è­‰é›†: {len(val_images)} å¼µ")


if __name__ == '__main__':
    main()
```

### è³‡æ–™å¢å¼·ç­–ç•¥

```python
# åœ¨ yolo12_train.py ä¸­é…ç½®
augmentations = dict(
    # å¹¾ä½•è®Šæ›
    degrees=5.0,          # æ—‹è½‰ Â±5 åº¦ï¼ˆæ¨¡æ“¬æ‹ç…§è§’åº¦ï¼‰
    translate=0.1,        # å¹³ç§» 10%
    scale=0.2,            # ç¸®æ”¾ Â±20%
    shear=2.0,           # å‰ªåˆ‡è®Šæ›
    perspective=0.0001,  # é€è¦–è®Šæ›ï¼ˆè¼•å¾®ï¼‰

    # é¡è‰²èª¿æ•´
    hsv_h=0.015,         # è‰²èª¿èª¿æ•´
    hsv_s=0.5,           # é£½å’Œåº¦èª¿æ•´
    hsv_v=0.4,           # äº®åº¦èª¿æ•´

    # æ¨¡ç³Šèˆ‡é›œè¨Š
    blur=0.001,          # è¼•å¾®æ¨¡ç³Šï¼ˆæ¨¡æ“¬å°ç„¦ä¸æº–ï¼‰

    # ç¿»è½‰ï¼ˆä¸å•Ÿç”¨ï¼Œæ¨‚è­œä¸æ‡‰è©²ç¿»è½‰ï¼‰
    fliplr=0.0,
    flipud=0.0,

    # Mosaic å¢å¼·ï¼ˆYOLO ç‰¹è‰²ï¼‰
    mosaic=0.5,          # 50% æ©Ÿç‡ä½¿ç”¨ mosaic
    mixup=0.1,           # 10% æ©Ÿç‡ä½¿ç”¨ mixup
)
```

---

## æ¨¡å‹è¨“ç·´

### è¨“ç·´ç’°å¢ƒè¨­ç½®

å»ºç«‹ `training/requirements-train.txt`:

```
# æ·±åº¦å­¸ç¿’æ¡†æ¶
torch>=2.1.0
torchvision>=0.16.0

# Ultralytics YOLO12
ultralytics>=8.3.0

# è³‡æ–™è™•ç†
numpy>=1.24.0
opencv-python>=4.8.0
Pillow>=10.0.0
pandas>=2.0.0

# è¦–è¦ºåŒ–
matplotlib>=3.7.0
seaborn>=0.12.0
tensorboard>=2.14.0

# å·¥å…·
tqdm>=4.65.0
PyYAML>=6.0
scikit-learn>=1.3.0

# TFLite è½‰æ›
tensorflow>=2.14.0
onnx>=1.15.0
onnxruntime>=1.16.0
```

å®‰è£:

```bash
cd training
pip install -r requirements-train.txt
```

### YOLO12s è¨“ç·´è…³æœ¬

å»ºç«‹ `training/yolo12_train.py`:

```python
"""
YOLO12 å››éƒ¨å’Œè²æ¨‚è­œè¾¨è­˜è¨“ç·´è…³æœ¬
ç¡¬é«”éœ€æ±‚: RTX 5060 (8GB VRAM)
é ä¼°æ™‚é–“: 200 epochs Ã— 8-10 hours = ç´„ 2 å¤©
"""
from ultralytics import YOLO
from pathlib import Path
import torch
import yaml
from datetime import datetime

# ============= é…ç½®å€ =============

# ç¡¬é«”é…ç½®
DEVICE = 0  # GPU 0 (RTX 5060)
WORKERS = 8  # è³‡æ–™è¼‰å…¥ç·šç¨‹æ•¸

# è¨“ç·´è¶…åƒæ•¸
BATCH_SIZE = 16  # RTX 5060 8GB å¯ç”¨ batch size
IMG_SIZE = 640   # YOLO æ¨™æº–è¼¸å…¥å°ºå¯¸
EPOCHS = 250     # YOLO12 éœ€è¦æ›´é•·è¨“ç·´æ™‚é–“
PATIENCE = 50    # Early stopping patience

# å­¸ç¿’ç‡ç­–ç•¥
LR0 = 0.01       # åˆå§‹å­¸ç¿’ç‡
LRF = 0.01       # æœ€çµ‚å­¸ç¿’ç‡ï¼ˆç·šæ€§è¡°æ¸›ï¼‰

# æ¨¡å‹é¸æ“‡
MODEL_VARIANT = 'yolo12s'  # æˆ– 'yolo12n' ç”¨æ–¼å‚™æ´

# è·¯å¾‘é…ç½®
DATASET_YAML = 'omr_harmony.yaml'
PROJECT_NAME = 'harmony_omr'
RUN_NAME = f'{MODEL_VARIANT}_{datetime.now().strftime("%Y%m%d_%H%M%S")}'

# ============= è¨“ç·´æµç¨‹ =============

def check_environment():
    """æª¢æŸ¥è¨“ç·´ç’°å¢ƒ"""
    print("=== ç’°å¢ƒæª¢æŸ¥ ===")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU åç¨±: {torch.cuda.get_device_name(0)}")
        print(f"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print()


def load_dataset_config():
    """è¼‰å…¥ä¸¦é©—è­‰è³‡æ–™é›†é…ç½®"""
    with open(DATASET_YAML, 'r') as f:
        config = yaml.safe_load(f)

    print("=== è³‡æ–™é›†é…ç½® ===")
    print(f"è¨“ç·´é›†: {config['train']}")
    print(f"é©—è­‰é›†: {config['val']}")
    print(f"é¡åˆ¥æ•¸: {config['nc']}")
    print(f"é¡åˆ¥åç¨±: {config['names'][:5]}... (å…± {len(config['names'])} é¡)")
    print()

    return config


def train_yolo12():
    """è¨“ç·´ YOLO12 æ¨¡å‹"""
    print(f"=== é–‹å§‹è¨“ç·´ {MODEL_VARIANT.upper()} ===")
    print(f"å°ˆæ¡ˆ: {PROJECT_NAME}")
    print(f"åŸ·è¡Œ: {RUN_NAME}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Epochs: {EPOCHS}")
    print()

    # è¼‰å…¥é è¨“ç·´æ¨¡å‹
    model = YOLO(f'{MODEL_VARIANT}.pt')

    # é–‹å§‹è¨“ç·´
    results = model.train(
        # è³‡æ–™é…ç½®
        data=DATASET_YAML,

        # è¨“ç·´è¶…åƒæ•¸
        epochs=EPOCHS,
        batch=BATCH_SIZE,
        imgsz=IMG_SIZE,

        # å­¸ç¿’ç‡
        lr0=LR0,
        lrf=LRF,

        # å„ªåŒ–å™¨
        optimizer='AdamW',  # YOLO12 æ¨è–¦
        weight_decay=0.0005,

        # Early stopping
        patience=PATIENCE,

        # è³‡æ–™å¢å¼·ï¼ˆåƒè€ƒå‰é¢çš„é…ç½®ï¼‰
        degrees=5.0,
        translate=0.1,
        scale=0.2,
        shear=2.0,
        perspective=0.0001,
        hsv_h=0.015,
        hsv_s=0.5,
        hsv_v=0.4,
        blur=0.001,
        mosaic=0.5,
        mixup=0.1,

        # ç¡¬é«”é…ç½®
        device=DEVICE,
        workers=WORKERS,

        # è¼¸å‡ºé…ç½®
        project=PROJECT_NAME,
        name=RUN_NAME,
        exist_ok=False,

        # å„²å­˜è¨­å®š
        save=True,
        save_period=10,  # æ¯ 10 epochs å„²å­˜ä¸€æ¬¡

        # é©—è­‰è¨­å®š
        val=True,

        # å…¶ä»–
        verbose=True,
        seed=42,
    )

    print("\n=== è¨“ç·´å®Œæˆ ===")
    print(f"æœ€ä½³æ¨¡å‹: {PROJECT_NAME}/{RUN_NAME}/weights/best.pt")
    print(f"æœ€çµ‚ mAP@0.5: {results.results_dict['metrics/mAP50(B)']:.4f}")
    print(f"æœ€çµ‚ mAP@0.5:0.95: {results.results_dict['metrics/mAP50-95(B)']:.4f}")

    return results


def validate_model():
    """é©—è­‰æœ€ä½³æ¨¡å‹"""
    print("\n=== æ¨¡å‹é©—è­‰ ===")

    best_model_path = f"{PROJECT_NAME}/{RUN_NAME}/weights/best.pt"
    model = YOLO(best_model_path)

    # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°
    metrics = model.val(
        data=DATASET_YAML,
        imgsz=IMG_SIZE,
        batch=BATCH_SIZE,
        device=DEVICE,
    )

    print(f"é©—è­‰ mAP@0.5: {metrics.box.map50:.4f}")
    print(f"é©—è­‰ mAP@0.5:0.95: {metrics.box.map:.4f}")
    print(f"é©—è­‰ Precision: {metrics.box.mp:.4f}")
    print(f"é©—è­‰ Recall: {metrics.box.mr:.4f}")

    return metrics


def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    check_environment()
    load_dataset_config()

    # è¨“ç·´
    results = train_yolo12()

    # é©—è­‰
    metrics = validate_model()

    print("\n=== å…¨éƒ¨å®Œæˆ ===")
    print("ä¸‹ä¸€æ­¥: åŸ·è¡Œ export_models.py é€²è¡Œæ¨¡å‹åŒ¯å‡ºèˆ‡é‡åŒ–")


if __name__ == '__main__':
    main()
```

### è³‡æ–™é›†é…ç½®æª”æ¡ˆ

å»ºç«‹ `training/omr_harmony.yaml`:

```yaml
# YOLO12 å››éƒ¨å’Œè²è³‡æ–™é›†é…ç½®

# è·¯å¾‘ï¼ˆç›¸å°æ–¼æ­¤ yaml æª”æ¡ˆï¼‰
path: ./datasets/yolo_harmony
train: images/train
val: images/val
test: images/test  # å¯é¸

# é¡åˆ¥æ•¸é‡
nc: 20

# é¡åˆ¥åç¨±ï¼ˆç´¢å¼•å°æ‡‰ convert_dataset.py ä¸­çš„ HARMONY_CLASSESï¼‰
names:
  0: notehead_filled
  1: notehead_hollow
  2: stem_up
  3: stem_down
  4: beam
  5: flag
  6: clef_treble
  7: clef_bass
  8: clef_alto
  9: clef_tenor
  10: accidental_sharp
  11: accidental_flat
  12: accidental_natural
  13: rest_quarter
  14: rest_half
  15: rest_whole
  16: barline
  17: time_signature
  18: key_signature
  19: staffline
```

### ä¸¦è¡Œè¨“ç·´ YOLO12nï¼ˆå‚™æ´ï¼‰

å»ºç«‹ `training/train_both.sh`:

```bash
#!/bin/bash
# ä¸¦è¡Œè¨“ç·´ YOLO12s å’Œ YOLO12n

echo "é–‹å§‹ä¸¦è¡Œè¨“ç·´ YOLO12s å’Œ YOLO12n..."

# è¨“ç·´ YOLO12sï¼ˆä¸»æ¨¡å‹ï¼‰
python yolo12_train.py --model yolo12s &
PID_S=$!

# ç­‰å¾… 1 å°æ™‚ï¼Œè®“ YOLO12s å…ˆä½¿ç”¨å®Œæ•´ GPU
sleep 3600

# è¨“ç·´ YOLO12nï¼ˆå‚™æ´ï¼Œbatch size æ›´å¤§ï¼‰
python yolo12_train.py --model yolo12n --batch 24 &
PID_N=$!

# ç­‰å¾…å…©å€‹è¨“ç·´éƒ½å®Œæˆ
wait $PID_S
wait $PID_N

echo "å…©å€‹æ¨¡å‹è¨“ç·´å®Œæˆï¼"
```

---

## æ¨¡å‹é‡åŒ–èˆ‡åŒ¯å‡º

### TFLite INT8 é‡åŒ–æµç¨‹

å»ºç«‹ `training/export_models.py`:

```python
"""
YOLO12 æ¨¡å‹åŒ¯å‡ºèˆ‡ INT8 é‡åŒ–
è¼¸å‡º: yolo12s_int8.tflite, yolo12n_int8.tflite
"""
from ultralytics import YOLO
from pathlib import Path
import tensorflow as tf
import numpy as np
from PIL import Image

# ============= é…ç½® =============

MODELS_TO_EXPORT = [
    'harmony_omr/yolo12s_20251120_XXXXXX/weights/best.pt',  # æ›¿æ›ç‚ºå¯¦éš›è·¯å¾‘
    'harmony_omr/yolo12n_20251120_YYYYYY/weights/best.pt',
]

OUTPUT_DIR = Path('../android-app/app/src/main/assets/models')
IMG_SIZE = 640

# ============= é‡åŒ–ç”¨ä»£è¡¨æ€§è³‡æ–™é›† =============

def representative_dataset_gen():
    """
    æä¾›ä»£è¡¨æ€§è³‡æ–™é›†ç”¨æ–¼ INT8 é‡åŒ–
    å¾é©—è­‰é›†éš¨æ©ŸæŠ½å– 100 å¼µåœ–ç‰‡
    """
    dataset_root = Path('datasets/yolo_harmony/images/val')
    image_files = list(dataset_root.glob('*.png'))[:100]

    for img_path in image_files:
        # è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
        img = Image.open(img_path).convert('RGB')
        img = img.resize((IMG_SIZE, IMG_SIZE))
        img_array = np.array(img, dtype=np.float32) / 255.0
        img_array = np.expand_dims(img_array, axis=0)  # (1, 640, 640, 3)

        yield [img_array]


# ============= åŒ¯å‡ºæµç¨‹ =============

def export_to_tflite_int8(model_path: str, output_name: str):
    """åŒ¯å‡ºç‚º TFLite INT8 é‡åŒ–æ¨¡å‹"""
    print(f"\n=== åŒ¯å‡º {model_path} ===")

    # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
    model = YOLO(model_path)

    # Step 1: åŒ¯å‡ºç‚º TFLite (FP32)
    print("Step 1: åŒ¯å‡º FP32 TFLite...")
    model.export(
        format='tflite',
        imgsz=IMG_SIZE,
        int8=False,  # å…ˆä¸é‡åŒ–
    )

    fp32_path = model_path.replace('.pt', '_saved_model/best_float32.tflite')

    # Step 2: è½‰æ›ç‚º INT8
    print("Step 2: INT8 é‡åŒ–...")
    converter = tf.lite.TFLiteConverter.from_saved_model(
        model_path.replace('.pt', '_saved_model')
    )

    # å•Ÿç”¨ INT8 é‡åŒ–
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8  # è¼¸å…¥ä¹Ÿé‡åŒ–
    converter.inference_output_type = tf.uint8  # è¼¸å‡ºä¹Ÿé‡åŒ–

    tflite_quant_model = converter.convert()

    # å„²å­˜
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_path = OUTPUT_DIR / f"{output_name}_int8.tflite"

    with open(output_path, 'wb') as f:
        f.write(tflite_quant_model)

    # æ¯”è¼ƒæª”æ¡ˆå¤§å°
    import os
    fp32_size = os.path.getsize(fp32_path) / 1e6
    int8_size = os.path.getsize(output_path) / 1e6
    compression_ratio = fp32_size / int8_size

    print(f"âœ… åŒ¯å‡ºå®Œæˆ: {output_path}")
    print(f"   FP32 å¤§å°: {fp32_size:.2f} MB")
    print(f"   INT8 å¤§å°: {int8_size:.2f} MB")
    print(f"   å£“ç¸®æ¯”: {compression_ratio:.2f}x")

    return output_path


def validate_tflite_model(tflite_path: Path):
    """é©—è­‰ TFLite æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨è«–"""
    print(f"\n=== é©—è­‰ {tflite_path.name} ===")

    # è¼‰å…¥æ¨¡å‹
    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))
    interpreter.allocate_tensors()

    # å–å¾—è¼¸å…¥/è¼¸å‡ºè©³æƒ…
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(f"è¼¸å…¥ shape: {input_details[0]['shape']}")
    print(f"è¼¸å…¥ dtype: {input_details[0]['dtype']}")
    print(f"è¼¸å‡º shape: {output_details[0]['shape']}")
    print(f"è¼¸å‡º dtype: {output_details[0]['dtype']}")

    # æ¸¬è©¦æ¨è«–ï¼ˆéš¨æ©Ÿè¼¸å…¥ï¼‰
    test_input = np.random.randint(0, 256, size=input_details[0]['shape'], dtype=np.uint8)
    interpreter.set_tensor(input_details[0]['index'], test_input)

    import time
    start = time.time()
    interpreter.invoke()
    end = time.time()

    output = interpreter.get_tensor(output_details[0]['index'])

    print(f"âœ… æ¨è«–æˆåŠŸ")
    print(f"   æ¨è«–æ™‚é–“: {(end - start) * 1000:.2f} ms (CPU)")
    print(f"   è¼¸å‡ºç¯„åœ: [{output.min()}, {output.max()}]")


def main():
    """ä¸»åŒ¯å‡ºæµç¨‹"""
    print("=== YOLO12 æ¨¡å‹åŒ¯å‡ºèˆ‡é‡åŒ– ===\n")

    exported_models = []

    # åŒ¯å‡º YOLO12s
    if len(MODELS_TO_EXPORT) > 0:
        path = export_to_tflite_int8(MODELS_TO_EXPORT[0], 'yolo12s')
        validate_tflite_model(path)
        exported_models.append(path)

    # åŒ¯å‡º YOLO12n
    if len(MODELS_TO_EXPORT) > 1:
        path = export_to_tflite_int8(MODELS_TO_EXPORT[1], 'yolo12n')
        validate_tflite_model(path)
        exported_models.append(path)

    print("\n=== å…¨éƒ¨å®Œæˆ ===")
    print(f"å·²åŒ¯å‡º {len(exported_models)} å€‹æ¨¡å‹:")
    for p in exported_models:
        print(f"  - {p}")
    print("\nä¸‹ä¸€æ­¥: å°‡ .tflite æª”æ¡ˆè¤‡è£½åˆ° android-app/app/src/main/assets/models/")


if __name__ == '__main__':
    main()
```

---

## Android æ•´åˆ

### æ›´æ–° build.gradle.kts

å°‡ TensorFlow Lite ä¾è³´åŠ å…¥ `android-app/app/build.gradle.kts`:

```kotlin
dependencies {
    // ... ç¾æœ‰ä¾è³´ ...

    // ========== TensorFlow Lite (YOLO12 æ¨è«–) ==========
    implementation("org.tensorflow:tensorflow-lite:2.14.0")
    implementation("org.tensorflow:tensorflow-lite-support:0.4.4")
    implementation("org.tensorflow:tensorflow-lite-gpu:2.14.0")  // GPU åŠ é€Ÿ
    implementation("org.tensorflow:tensorflow-lite-task-vision:0.4.4")  // è¦–è¦ºä»»å‹™å·¥å…·

    // NNAPI Delegate (NPU åŠ é€Ÿ)
    implementation("org.tensorflow:tensorflow-lite-select-tf-ops:2.14.0")
}
```

### Yolo12OmrClient.kt å¯¦ä½œ

å»ºç«‹ `android-app/app/src/main/java/com/example/harmonychecker/core/omr/Yolo12OmrClient.kt`:

```kotlin
package com.example.harmonychecker.core.omr

import android.content.Context
import android.graphics.Bitmap
import android.graphics.RectF
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.gpu.GpuDelegate
import org.tensorflow.lite.support.common.FileUtil
import org.tensorflow.lite.support.image.ImageProcessor
import org.tensorflow.lite.support.image.TensorImage
import org.tensorflow.lite.support.image.ops.ResizeOp
import java.nio.ByteBuffer
import kotlin.math.exp

/**
 * YOLO12 TFLite æ¨è«–å®¢æˆ¶ç«¯
 *
 * è² è²¬ï¼š
 * 1. è¼‰å…¥ TFLite æ¨¡å‹ï¼ˆINT8 é‡åŒ–ï¼‰
 * 2. åœ–åƒé è™•ç†
 * 3. åŸ·è¡Œæ¨è«–
 * 4. NMS å¾Œè™•ç†
 * 5. è¼¸å‡ºç¬¦è™Ÿæª¢æ¸¬çµæœ
 */
class Yolo12OmrClient(
    private val context: Context,
    private val modelVariant: ModelVariant = ModelVariant.YOLO12S,
    private val useGpuDelegate: Boolean = true
) : OmrClient {

    // æ¨¡å‹é…ç½®
    private val inputSize = 640
    private val numClasses = 20
    private val confidenceThreshold = 0.25f
    private val iouThreshold = 0.45f

    // TFLite Interpreter
    private var interpreter: Interpreter? = null
    private var gpuDelegate: GpuDelegate? = null

    // é¡åˆ¥åç¨±
    private val classNames = listOf(
        "notehead_filled", "notehead_hollow",
        "stem_up", "stem_down", "beam", "flag",
        "clef_treble", "clef_bass", "clef_alto", "clef_tenor",
        "accidental_sharp", "accidental_flat", "accidental_natural",
        "rest_quarter", "rest_half", "rest_whole",
        "barline", "time_signature", "key_signature", "staffline"
    )

    init {
        loadModel()
    }

    /**
     * è¼‰å…¥ TFLite æ¨¡å‹
     */
    private fun loadModel() {
        val modelPath = when (modelVariant) {
            ModelVariant.YOLO12S -> "models/yolo12s_int8.tflite"
            ModelVariant.YOLO12N -> "models/yolo12n_int8.tflite"
        }

        val options = Interpreter.Options().apply {
            setNumThreads(4)  // ä½¿ç”¨ 4 å€‹ CPU åŸ·è¡Œç·’

            if (useGpuDelegate) {
                try {
                    gpuDelegate = GpuDelegate()
                    addDelegate(gpuDelegate)
                } catch (e: Exception) {
                    android.util.Log.w("Yolo12Client", "GPU delegate åˆå§‹åŒ–å¤±æ•—ï¼Œé™ç´šåˆ° CPU", e)
                }
            }
        }

        val modelBuffer = FileUtil.loadMappedFile(context, modelPath)
        interpreter = Interpreter(modelBuffer, options)

        android.util.Log.i("Yolo12Client", "æ¨¡å‹è¼‰å…¥å®Œæˆ: $modelPath")
    }

    /**
     * å¯¦ä½œ OmrClient ä»‹é¢
     */
    override suspend fun recognizeScore(image: Bitmap): OmrResult {
        val detections = detect(image)

        // çµ„è£æˆæ¨‚è­œçµæ§‹ï¼ˆç”± SymbolAssembler è™•ç†ï¼‰
        val assembler = SymbolAssembler()
        val chordSnapshots = assembler.assemble(detections, image.width, image.height)

        return OmrResult(
            chords = chordSnapshots,
            keySignature = assembler.detectedKeySignature,
            timeSignature = assembler.detectedTimeSignature,
            raw = detections
        )
    }

    /**
     * YOLO12 æ¨è«–æ ¸å¿ƒé‚è¼¯
     */
    private fun detect(bitmap: Bitmap): List<Detection> {
        // Step 1: åœ–åƒé è™•ç†
        val inputTensor = preprocessImage(bitmap)

        // Step 2: åŸ·è¡Œæ¨è«–
        val outputTensor = runInference(inputTensor)

        // Step 3: å¾Œè™•ç†ï¼ˆNMSï¼‰
        val detections = postprocess(outputTensor, bitmap.width, bitmap.height)

        return detections
    }

    /**
     * åœ–åƒé è™•ç†
     */
    private fun preprocessImage(bitmap: Bitmap): ByteBuffer {
        // Resize to 640x640
        val resized = Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)

        // è½‰æ›ç‚º ByteBufferï¼ˆINT8 è¼¸å…¥ï¼‰
        val byteBuffer = ByteBuffer.allocateDirect(1 * inputSize * inputSize * 3)
        byteBuffer.rewind()

        val intValues = IntArray(inputSize * inputSize)
        resized.getPixels(intValues, 0, inputSize, 0, 0, inputSize, inputSize)

        for (pixelValue in intValues) {
            // æå– RGB ä¸¦è½‰ç‚º uint8 [0-255]
            byteBuffer.put(((pixelValue shr 16) and 0xFF).toByte())  // R
            byteBuffer.put(((pixelValue shr 8) and 0xFF).toByte())   // G
            byteBuffer.put((pixelValue and 0xFF).toByte())           // B
        }

        return byteBuffer
    }

    /**
     * åŸ·è¡Œæ¨è«–
     */
    private fun runInference(input: ByteBuffer): Array<Array<ByteArray>> {
        // YOLO12 è¼¸å‡º: [1, 8400, 84] (INT8)
        // 8400 = num_anchors, 84 = 4 (bbox) + 80 (classes)
        val output = Array(1) { Array(8400) { ByteArray(numClasses + 4) } }

        interpreter?.run(input, output)

        return output
    }

    /**
     * å¾Œè™•ç†ï¼šNMS + åº§æ¨™é‚„åŸ
     */
    private fun postprocess(
        output: Array<Array<ByteArray>>,
        originalWidth: Int,
        originalHeight: Int
    ): List<Detection> {
        val detections = mutableListOf<Detection>()

        // è§£æ YOLO è¼¸å‡º
        for (i in 0 until 8400) {
            val row = output[0][i]

            // è§£é‡åŒ–ï¼ˆINT8 â†’ FP32ï¼‰
            val dequantized = dequantize(row)

            // å–å¾— bbox å’Œä¿¡å¿ƒåº¦
            val centerX = dequantized[0]
            val centerY = dequantized[1]
            val width = dequantized[2]
            val height = dequantized[3]

            // å–å¾—æœ€é«˜ä¿¡å¿ƒåº¦çš„é¡åˆ¥
            val classScores = dequantized.sliceArray(4 until dequantized.size)
            val maxScore = classScores.maxOrNull() ?: 0f
            val classId = classScores.indexOf(maxScore)

            if (maxScore < confidenceThreshold) continue

            // è½‰æ›åº§æ¨™ï¼ˆå¾ 640x640 é‚„åŸåˆ°åŸå§‹å°ºå¯¸ï¼‰
            val scaleX = originalWidth.toFloat() / inputSize
            val scaleY = originalHeight.toFloat() / inputSize

            val bbox = RectF(
                (centerX - width / 2) * scaleX,
                (centerY - height / 2) * scaleY,
                (centerX + width / 2) * scaleX,
                (centerY + height / 2) * scaleY
            )

            detections.add(Detection(
                bbox = bbox,
                classId = classId,
                className = classNames[classId],
                confidence = maxScore
            ))
        }

        // NMS
        return nms(detections, iouThreshold)
    }

    /**
     * åé‡åŒ–ï¼ˆINT8 â†’ FP32ï¼‰
     */
    private fun dequantize(quantized: ByteArray): FloatArray {
        // ç°¡åŒ–ç‰ˆï¼šå‡è¨­ scale = 1/128, zero_point = 128
        return quantized.map { (it.toInt() and 0xFF) / 128f - 1f }.toFloatArray()
    }

    /**
     * Non-Maximum Suppression
     */
    private fun nms(detections: List<Detection>, iouThreshold: Float): List<Detection> {
        val sorted = detections.sortedByDescending { it.confidence }
        val selected = mutableListOf<Detection>()
        val suppressed = BooleanArray(sorted.size) { false }

        for (i in sorted.indices) {
            if (suppressed[i]) continue

            selected.add(sorted[i])

            for (j in i + 1 until sorted.size) {
                if (suppressed[j]) continue
                if (calculateIoU(sorted[i].bbox, sorted[j].bbox) > iouThreshold) {
                    suppressed[j] = true
                }
            }
        }

        return selected
    }

    /**
     * è¨ˆç®— IoU (Intersection over Union)
     */
    private fun calculateIoU(box1: RectF, box2: RectF): Float {
        val intersection = RectF(box1)
        if (!intersection.intersect(box2)) return 0f

        val intersectionArea = intersection.width() * intersection.height()
        val box1Area = box1.width() * box1.height()
        val box2Area = box2.width() * box2.height()
        val unionArea = box1Area + box2Area - intersectionArea

        return intersectionArea / unionArea
    }

    /**
     * æ¸…ç†è³‡æº
     */
    fun close() {
        interpreter?.close()
        gpuDelegate?.close()
    }

    enum class ModelVariant {
        YOLO12S,  // é«˜æº–ç¢ºåº¦ï¼Œ10MB
        YOLO12N   // è¼•é‡ç´šï¼Œ3MB
    }
}

/**
 * å–®ä¸€ç¬¦è™Ÿæª¢æ¸¬çµæœ
 */
data class Detection(
    val bbox: RectF,
    val classId: Int,
    val className: String,
    val confidence: Float
)

/**
 * OMR è¾¨è­˜çµæœ
 */
data class OmrResult(
    val chords: List<ChordSnapshot>,
    val keySignature: KeySignature?,
    val timeSignature: TimeSignature?,
    val raw: List<Detection>  // åŸå§‹æª¢æ¸¬çµæœï¼ˆç”¨æ–¼ debugï¼‰
)
```

---

## ç¬¦è™Ÿçµ„è£é‚è¼¯

*ï¼ˆç¹¼çºŒæ’°å¯« Symbol Assemblerã€å¤šè£ç½®é©é…ã€æ¸¬è©¦é©—è­‰ç­‰ç« ç¯€...ç”±æ–¼å­—æ•¸é™åˆ¶ï¼Œæˆ‘å…ˆå‰µå»ºé€™å€‹æª”æ¡ˆçš„ç¬¬ä¸€éƒ¨åˆ†ï¼‰*

---

**æœªå®Œå¾…çºŒ**ï¼šæœ¬æ–‡æª”å°‡æŒçºŒæ›´æ–°ï¼ŒåŒ…å«ï¼š
- Section 7: ç¬¦è™Ÿçµ„è£é‚è¼¯ï¼ˆSymbolAssembler.kt å®Œæ•´å¯¦ä½œï¼‰
- Section 8: å¤šè£ç½®é©é…ç­–ç•¥ï¼ˆå‹•æ…‹æ¨¡å‹é¸æ“‡ï¼‰
- Section 9: æ¸¬è©¦èˆ‡é©—è­‰
- Section 10: é¢¨éšªèˆ‡ç·©è§£

**ç•¶å‰ç‰ˆæœ¬**: 1.0-draft
**æœ€å¾Œæ›´æ–°**: 2025-11-20

# **運用深度學習方法之樂譜辨識 \- 完整逐字內容**

文件名稱： 運用深度學習方法之樂譜辨識.pdf  
說明： 本文件為原始 PDF 之完整逐字擷取，包含所有章節、圖片說明文字、表格數據及參考文獻。

## **PAGE 1 (封面)**

國立臺灣海洋大學

電機工程學系

碩士學位論文

指導教授:曾敬翔

運用深度學習方法之樂譜辨識

Deep Learning Approaches for Music  
Score Recognition  
研究生:曾鈺翔 撰  
中華民國113年6月

## **PAGE 2 (英文封面)**

運用深度學習方法之樂譜辨識

Deep Learning Approaches for Music Score

Recognition

研究生:曾鈺翔

指導教授:曾敬翔

Student: Yu-Tsiang Tseng  
Advisor: Ching-Hsiang Tseng  
國立臺灣海洋大學

電機工程學系

碩士論文

A Thesis

Submitted to Department of Electrical Engineering  
College of Electrical Engineering and Computer Science  
National Taiwan Ocean University

in partial fulfillment of the requirements

for the Degree of

Master of Science

in

Electrical Engineering

June 2024

Keelung, Taiwan, Republic of China

中華民國113年6月

## **PAGE 3 (致謝)**

致謝

在完成這篇論文的過程中,我想要向許多人表達我的感激之情,他們的幫  
助和支持對我完成這項工作起到了至關重要的作用。首先,我要衷心感謝我的  
指導教授曾敬翔博士,您的專業知識、耐心和激勵對我有莫大的幫助,感謝您  
在整個研究過程中給予我的指導和建議,您精闢的分析方式及找出問題的犀利  
眼光使我受益良多,讓我的解決問題的能力更上一層樓。此外我還要感謝兩位  
口委:張順雄教授及盧晃瑩教授,您們在口試期間的建議對我的研究起到了關  
鍵作用,讓我的論文方法和內容有了更精準的調整和改進。同時在研究方面,  
我要感謝實驗室的學長佐惠和祐寧,幫助我對研究內容有更深刻的理解,還有  
感謝學弟妹士勛、黃元、睿桓和雅竹給予的意見和幫助,最後還有同屆同學乙  
成的相互砥礪和各種實驗室事項的大力協助,讓我的研究之路能更加順暢,感  
謝你們在我需要幫助時在我身旁提點我,我很感激有你們作為我的同伴,和你  
們一起度過了這段寶貴的時光。最後,我要感謝所有在這項研究中給予支持  
的人們,雖然無法一一列舉,但你們的幫助對我的研究和寫作都起到了關鍵作  
用,非常感激你們的幫助與陪伴。

## **PAGE 4 (摘要)**

摘要

光學樂譜辨識是被廣泛應用的樂譜影像辨識技術,它旨在將印刷或手寫的  
樂譜轉換成計算機可理解的形式,為音樂數據的自動處理和分析提供了基礎,  
且相較於紙本更不容易損壞。回顧樂譜辨識的歷史發展,可以發現傳統方法在  
處理複雜的音樂符號結構時存在一定的局限性。深度學習是一種機器學習的  
分支,通過模擬人類大腦的工作方式,實現了對複雜數據的高效學習,為解決  
各種現實世界的問題提供了強大的幫助。它在許多領域的應用皆取得良好的效  
果,成功的案例包括有語音識別、圖像識別、自然語言處理等。本研究提出了  
一種用於樂譜辨識的深度學習方法,此方法為先利用光學樂譜辨識進行樂譜的  
前置處理,將樂譜中的各種音符偵測並剪取出來,再將這些剪取出來的音符圖  
片用三種深度學習的模型(GoogLeNet、ResNet和Inception-v3)進行辨識與分類。  
整體音符的辨識率依序分別有97.2%、92.21%和95.95%,這與參考文獻\[1\]的辨  
識成功率(49.67%)相比皆有明顯提升。這意味著所提出的方法能夠更正確地判斷  
音符的播放時長。緊接著可以藉由音符的位置資訊來判斷音高,最後再將所有  
資訊統合並輸出成音檔。本論文提出的研究方法為樂譜辨識領域帶來了新的思  
路和嘗試,它相較於傳統的光學樂譜辨識有更高的靈活性,這對於促進樂譜信  
息處理和發展音樂智能化技術的發展具有重要意義。  
關鍵詞:光學樂譜辨識、深度學習、機器學習、人工神經網絡、影像辨識、遷  
移學習  
I

## **PAGE 5 (Abstract)**

Abstract

Optical music recognition (OMR) is a widely applied technique in the image  
recognition of musical scores, aiming to convert printed or handwritten music scores into  
a computer-understandable format, thereby providing the foundation for the automatic  
processing and analysis of musical data. Compared to paper-based scores, OMR is  
less susceptible to damage. By reviewing the historical development of music score  
recognition, one can find that traditional approaches have certain limitations in handling  
complex musical symbol structures. Deep learning, a branch of machine learning,  
efficiently learns complex data patterns by simulating the workings of the human brain,  
offering powerful assistance in solving various real-world problems. It has shown  
promising results across fields such as speech recognition, image recognition, natural  
language processing, and so on. This study proposes a deep learning method for  
musical score recognition. The method first utilizes OMR for the preprocessing of  
the score, detecting and extracting various notes from the sheet music. The extracted  
images of the notes are then recognized and classified using three deep learning models  
(GoogLeNet, ResNet and Inception-v3). The overall note recognition rates for the  
three methods are 97.2%, 92.21%, and 95.95% respectively, which are significantly  
better compared to the recognition rate of 49.67% reported in the reference\[1\].This  
implies that the proposed method allows a more accurate determination of the note  
durations. Subsequently, the pitch is determined based on the positional information  
of the notes, and finally, all the information is integrated and exported to an audio file.  
The proposed research methodology brings new ideas and attempts to the field of music  
score recognition. Compared to the traditional OMR, it offers greater flexibility. This  
holds great significance for advancing music information processing and developing of  
intelligent music technologies.  
Keywords: optical music recognition, deep learning, machine learning, artificial neural  
network, image recognition, transfer learning  
II

## **PAGE 6 (目次 1/2)**

目次

| 章節 | 標題 | 頁碼 |
| :---- | :---- | :---- |
|  | 摘要 | I |
|  | Abstract | II |
|  | 目次 | III |
|  | 圖目次 | IV |
|  | 表目次 | VI |
| **第一章** | **緒論** | **1** |
| 1.1 | 引言 | 1 |
| 1.2 | 樂譜辨識對音樂領域的影響 | 2 |
| 1.3 | 深度學習與樂譜辨識之結合 | 2 |
| 1.4 | 論文架構 | 3 |
| **第二章** | **光學樂譜辨識** | **4** |
| 2.1 | 理論介紹及架構 | 4 |
| 2.2 | 影像前置處理 | 5 |
| 2.3 | 音樂符號辨識 | 7 |
| 2.4 | 樂譜重建 | 8 |
| 2.5 | 符號格式構建 | 9 |
| **第三章** | **深度學習** | **10** |
| 3.1 | 背景知識 | 10 |
| 3.2 | 在OMR中的發展 | 11 |
| 3.3 | 遷移學習 | 12 |
| 3.4 | 深度學習工具 | 13 |
| 3.4.1 | GoogLeNet | 13 |
| 3.4.2 | ResNet | 13 |
| 3.4.3 | Inception-v3 | 14 |
| **第四章** | **深度學習與OMR之結合** | **15** |
| 4.1 | 系統整體流程 | 15 |
| 4.2 | 影像前置處理 | 18 |

III

## **PAGE 7 (目次 2/2)**

| 章節 | 標題 | 頁碼 |
| :---- | :---- | :---- |
| 4.3 | 音樂符號辨識 | 19 |
| 4.3.1 | 音符圖片擷取 | 19 |
| 4.3.2 | 深度學習模型與音符分類器 | 24 |
| 4.3.3 | 目標樂譜排序 | 27 |
| 4.4 | 樂譜重建 | 28 |
| 4.5 | 符號格式構建 | 29 |
| 4.6 | 深度學習評估方式 | 30 |
| **第五章** | **辨識音符結果與比較** | **32** |
| 5.1 | 使用 GoogLeNet 辨識結果 | 32 |
| 5.2 | 使用 ResNet 辨識結果 | 33 |
| 5.3 | 使用 Inception-v3 辨識結果 | 34 |
| 5.4 | 模型辨識結果比對與分析 | 35 |
| 5.5 | 本論文與相關文獻之OMR系統比較 | 37 |
| **第六章** | **結論及未來展望** | **39** |
| 6.1 | 結論 | 39 |
| 6.2 | 未來展望 | 39 |
|  | 參考文獻 | 40 |

IV

## **PAGE 8 (圖目次 1/2)**

圖目次

| 圖號 | 圖標題 | 頁碼 |
| :---- | :---- | :---- |
| 圖1 | 光學樂譜辨識通用框架\[6\] | 5 |
| 圖2 | 五線譜線條(staffline)和間距(staffspace)之長度,穿過五線譜之細實線為圖3中的方塊序列 | 6 |
| 圖3 | 參考長度選定流程\[13\] | 6 |
| 圖4 | 將五線譜圖像進行黑色像素之水平投影,下圖的最大值的部分為譜線之位置 | 7 |
| 圖5 | 遷移學習示意圖 | 12 |
| 圖6 | 音符的深度學習模型訓練架構圖 | 17 |
| 圖7 | 將(a)圖中的樂譜使用像素的水平投影後以直方圖(b)表示,(a)圖中的紅線位置就是(b)圖中的波峰位置 | 19 |
| 圖8 | 定位五線譜間中線之方式,圖為將樂譜放大至兩個五線譜來分析,虛線為表示對應空白寬度大小 | 20 |
| 圖9 | 實線為高低音譜的中心線,虛線為段落之間的中心線 | 21 |
| 圖10 | (a)為五線譜段落原圖,(b)為去除掉五線譜後結果,(c)為去除小節線並補回音符空隙 | 22 |
| 圖11 | 在每個小節執行霍夫變換抓取音符頭 | 23 |
| 圖12 | 對每個音進行方框切割 | 23 |
| 圖13 | 找出A、B、C三部分並計算黑色像素數量,B為最大區域故保留 | 24 |
| 圖14 | 深度學習模型的訓練集 | 25 |
| 圖15 | 深度學習模型的驗證集 | 25 |
| 圖16 | 深度神經網絡訓練過程 | 26 |
| 圖17 | 音符圖片排序,(a)為高音譜圖片資料夾;(b)為低音譜圖片資料夾 | 27 |
| 圖18 | A的音高判斷方式 | 28 |
| 圖19 | 五線譜對應位置之音符音名與其對應的MIDI音高 | 30 |
| 圖20 | 參數在多目標混淆矩陣之位置,圓圈表示當前計算種類 | 31 |

V

## **PAGE 9 (圖目次 2/2)**

| 圖號 | 圖標題 | 頁碼 |
| :---- | :---- | :---- |
| 圖21 | GoogLeNet的辨識結果在混淆矩陣中的展示 | 32 |
| 圖22 | ResNet的辨識結果在混淆矩陣中的展示 | 33 |
| 圖23 | Inception-v3的辨識結果在混淆矩陣中的展示 | 34 |

VI

## **PAGE 10 (表目次)**

表目次

| 表號 | 表標題 | 頁碼 |
| :---- | :---- | :---- |
| 表1 | 相關的OMR軟體和程式\[6\] | 9 |
| 表2 | 創建 MIDI 樂譜格式 | 29 |
| 表3 | GoogLeNet的整體準確率和各分類的精確性和召回率及加權平均 | 32 |
| 表4 | ResNet的整體準確率和各分類的精確性和召回率 | 33 |
| 表5 | Inception-v3的整體準確率和各分類的精確性和召回率 | 34 |
| 表6 | 統整三種模型的整體準確率和各分類的精確性和召回率 | 35 |
| 表7 | 本論文與參考文獻\[1\]的OMR實作方法與結果之比較 | 38 |

VII

## **PAGE 11**

第一章 緒論

1.1 引言

樂譜是一種以用符號來記錄音樂的方法,它是音樂創作、演奏和傳播的重  
要媒介,使得音樂能夠被準確地傳達和理解。透過這些符號,它能夠提供豐富  
的信息,幫助演奏者準確地演奏出作曲家所期望的音樂效果。早期的樂譜主要  
以紙張手動抄寫,現在則有電腦程式可以輔助製作樂譜。  
樂譜之所以如此重要,不外乎於它的精確性,能夠準確地記錄音樂的節奏  
和音高,使得演奏者能夠盡可能地按照作曲者的意圖演奏音樂。樂譜還具有極  
大的普遍性,被廣泛地使用於音樂教育、演奏和創作及其相關領域中,其中不  
能不提到的還有樂譜的靈活性,能夠用於獨奏、合奏以及伴奏等不同形式之演  
奏。  
雖然樂譜有如此多的優點,但學習鋼琴樂譜仍需要花費大量的時間和精  
力,尤其對於初學者來說,需要掌握音符、節奏、指法等大量基本知識才能彈  
奏出悅耳的音樂,而且樂譜的準確性卻也限制了情感方面的表現,只能靠表演  
者自身的經驗去彌補。  
因此光學樂譜辨識(optical music recognition, OMR)是一項重要而具有挑戰  
性的研究,讓音樂的外行人也能夠藉此快速的演奏出音樂,這在音樂信息檢索  
和音樂教育等相關領域具有廣泛的應用價值。隨著深度學習(deep learning)技術  
的不斷發展,光學樂譜辨識也進入了一個全新的階段,本文旨在探討並評估深  
度學習方法在光學樂譜辨識中的應用,並提出相應的解決方案以應對現有的挑  
戰。  
1

## **PAGE 12**

1.2 樂譜辨識對音樂領域的影響

樂譜隨著時代的演進,從最開始用手寫在紙張保存,到如今使用電腦製作  
並儲存成圖檔,樂譜的儲存形式已經越來越往電子化發展,而接下來更進一步  
的儲存方式便是直接將樂譜的資訊轉換成機器可讀取的格式,但是僅僅只儲存  
樂譜的圖檔並不能讓機器也理解這些符號的意思,於是樂譜辨識這項系統應運  
而生。  
光學樂譜辨識的目的在於利用計算機視覺技術及機器學習算法,將紙上或  
影像中的樂譜轉換為數字或音訊表示,以便進行自動化的音樂分析、樂曲重  
製、樂譜編輯等應用。這項技術可以幫助音樂家、作曲家、樂譜編輯者等音樂  
相關工作者節省大量時間和精力,提高工作效率,同時也為音樂學習提供了新  
的可能性,使得初學者可以更直觀地理解和學習樂曲。  
1.3 深度學習與樂譜辨識之結合

本論文之核心內容是依靠深度學習這項工具,利用它強大的影像學習能  
力,來幫助光學樂譜辨識的系統在各項音樂符號識別中加強其效能,主要的研  
究方式是先藉由光學樂譜辨識這項系統處理樂譜,這部分有兩個步驟主要是參  
考 Andy Zeng\[1\] 所提出的文獻,首先對樂譜的影像進行水平投影,先偵測出五  
線譜的位置並將其去除,之後再利用霍夫變換(Hough transform)去提取具有圓形  
特徵的物體\[2\],這步驟主要是去抓取樂譜上的音符。  
接下來利用偵測到的符號根據其前後符號的遠近,把符號從原始樂譜上抓  
取出來,並對這些圖片進行去除污漬的處理,讓圖片的辨識效果不會被那些  
錯誤的特徵誤導,接下來就先進行深度學習模型之訓練,本研究選擇三種模  
型來進行比對,利用處理過後的圖片放入至三種模型訓練完後,再將其遷移  
學習(transfer learning)\[33\]之結果拿來判斷符號種類,並會對各種模型和Andy  
Zeng\[1\] 所提出的系統進行錯誤率之比對與分析,最後本研究將各項資訊統合輸  
出成音樂數位介面(musical instrument digital interface, MIDI)之音檔,可以更理解  
各種深度學習模型對於光學樂譜辨識之效果。  
2

## **PAGE 13**

1.4 論文架構

·第一章介紹樂譜在音樂領域的重要性,以及光學樂譜辨識對於樂譜儲存格  
式的演進有重大意義,還有本論文將深度學習運用在光學樂譜辨識之動機  
與目的。  
·第二章介紹光學樂譜辨識這項系統的背景知識,並搭配各種文獻的演進去  
說明其中各項步驟的功用,最後在總結分析各文獻對於光學樂譜發展的意  
義。  
第三章介紹深度學習的背景知識與發展來源,其中特別針對深度學習與光  
學樂譜辨識結合的各種文獻進行解說,並且概述本研究使用到的三種深度  
學習模型。  
·第四章介紹本研究如何將深度學習應用於光學樂譜辨識中的詳細實作方  
法,主要步驟就是五線譜偵測與移除、音符偵測與切割、深度學習模型訓  
練、音樂符號辨識、音高判斷和輸出音檔。  
·第五章介紹未加入深度學習之OMR的模型與加入深度學習之OMR的三種  
模型之間的辨識結果與比較。  
·第六章介紹本研究將深度學習應用於光學樂譜辨識的結論,還有系統未來  
能夠改進與修正的地方。  
3

## **PAGE 14**

2.1 理論介紹及架構

第二章 光學樂譜辨識

光學樂譜辨識(optical music recognition, OMR)為一項音樂相關的研究領  
域,旨在探討如何以電子計算方式讀取文件中的音樂樂譜的圖片檔案\[3\]。這項  
研究的目標是教導計算機讀取和解釋樂譜各種符號的含義,並生成樂譜的機器  
可讀版本,這樣一來音樂便可以使用常見的文件格式保存,像是用於播放音檔  
的音樂數位介面(musical instrument digital interface, MIDI) \[4\],還有用在樂譜  
頁面布局的MusicXML。光學樂譜辨識也因名稱相似而曾被誤導地稱作光學字元  
辨識(optical character recognition, OCR)\[5\],但兩者的核心概念存在顯著差異,因  
此不該再使用該術語。  
這項技術的主要框架是在2012年由Ana Rebelo等人\[6\]所提出,他們對已  
發表的論文進行了分類與探討,詳細流程如圖1所示,在圖1中OMR 流程  
歸類為四個步驟:影像前置處理、音樂符號辨識、樂譜重建(musical notation  
reconstruction)和符號格式構建(final representation construction)。影像前置處理階  
段中根據研究的需求會對樂譜進行一些處理,像是有增強(enhancement)\[7\]、  
二值化(binarization)\[8\]、雜訊去除(noise removal) \[9\]、模糊(blurring)\[7\]、偏移糾  
正(de-skewing)\[10\]和形態學操作(morphological operations) \[7\],再來就是要獲取  
五線譜的參考長度(reference lengths);音樂符號辨識階段中會先進行五線譜的  
去除,再來對每個音樂符號進行分割,最後利用訓練好的分類器對符號進行分  
類;樂譜重建階段中是先將各個符號相對應的原本含義組合起來,然後創建出  
一個能表示出符號圖形之含義的規則;符號格式構建階段就是將樂譜轉換為機  
器可以讀取的符號格式。  
該框架為光學樂譜辨識這理論樹立一個統一的規則,至今的相關文獻大多  
數仍會參考此框架去設計光學樂譜辨識,雖然使用的專業術語略有不同,但大  
體上的步驟之區塊設計概念都是雷同的,而且該文獻也是目前被引用次數最多  
之論文,代表其在光學樂譜辨識舉足輕重的地位。  
4

## **PAGE 15**

圖1:光學樂譜辨識通用框架\[6\]

數位灰階樂譜 \-\> 影像前置處理 \-\> 音樂符號辨識 \-\> 樂譜重建 \-\> 符號格式構建 \-\> 音檔輸出

2.2 影像前置處理

在數字圖像處理中,就像所有信號處理系統一樣,可以根據需求應用  
不同的技術對輸入影像進行處理,使影像能以較高的品質進行檢測步驟。  
這樣做的目的是為了獲得更穩健和高效的識別過程。增強(enhancement)\[7\]、  
二值化(binarization)\[8\]、雜訊去除(noise removal) \[9\]、模糊(blurring)\[7\]、偏移糾  
正(de-skewing)\[10\]和形態學操作(morphological operations)\[7\]是預處理樂譜階段  
中的常被使用的一些技術。  
在這些可能會使用到的技術中,幾乎所有的光學樂譜辨識系統都以二值化  
過程開始,這意味著必須分析數位化的圖像,以確定哪些資訊是有用的(音樂  
符號和五線譜)以及那些是不必要的(背景和雜訊),因此二值化有很大的優  
點,可以通過減少需要處理的信息量來促進後續的任務,提升了運算效率,使  
得設計模型來應對光學樂譜辨識任務變得更容易,而且在二值化圖像中進行直  
線偵測以及符號的分割與識別比在灰階或彩色圖像中更加容易,所以這個方式  
被許多文獻所採用。  
接下來需要處理的部分就是找出樂譜中的參考長度(reference lengths),這是  
因為大多數的光學樂譜辨識都依賴於對五線譜線條(staffline)和相鄰兩線條之間  
距(staffspace)長度的估計(見圖2)。  
為了得到這個參考長度,許多文獻使用了運行長度編碼(run-length  
encoding, RLE) \[11\],這是一種非常簡單的數據壓縮形式,其中相同數據的  
5

## **PAGE 16**

圖2: 五線譜線條(staffline)和間距(staffspace)之長度,穿過五線譜之細實線為圖3中的方塊序列

\[圖示說明：展示 Staffspace Height 和 Staffline Height 的定義\]

運行被表示為單個數據值並計數,在作為識別過程輸入的二值圖像中,只  
會出現兩個值(1和0),使得運行長度編碼更加簡化,然後對樂譜的每一列進  
行編碼,最常出現的的黑色集合表示五線譜的線條長度,最常出現的白色  
集合表示五線譜的間距長度。接下來為了找出五線譜更具參考價值的長度資  
訊,Cardoso和Rebelo\[12\]根據\[13\]提出一種選取方式,通過兩個相鄰的運行長度  
編碼之和,並統計出哪個長度的出現次數最高,轉換成相對應的二維直方圖  
後,來更可靠地選取五線譜線條和間距的長度總和,詳細流程可參考圖3。  
↓

RLE=\[6,2,4,2,4,1,5,2,6,1\]  
sum=\[8,6,6,5,6,7,8,7\]  
↓  
histogram

1 2 3 4 5 6 7 8 9

圖3:參考長度選定流程\[13\]

6

## **PAGE 17**

圖3中顯示選定參考長度的範例,一開始的黑白方塊序列是圖2中的細實  
線,並將其調整為橫向方便圖片表示,此黑白方塊序列轉換為RLE數列後,第  
一位的6就是由六個連續白色的像素得出,第二位的2則是由兩個連續的黑色  
像素得出,以此類推;sum數列中第一位的8則是由RLE中的第一位加第二位  
之值,第二位的6是RLE中的第二位加第三位之值,以此類推;輸出sum之直  
方圖則顯示6出現的次數最高,故五線譜線條和間距的長度選定為6(虛線表  
示)。  
2.3 音樂符號辨識

在許多光學樂譜辨識系統中,樂譜譜線的偵測和移除是基本的步驟,會這  
樣做的原因在於需要將音樂符號隔離出來,以更有效和正確地偵測樂譜中的每  
個符號。儘管如此,有一些研究提出了不需要移除譜線的做法\[14\],在這種情況  
下,經常會導致其他音樂符號被分割,或者未移除的譜線片段被誤解為符號的  
一部分或新的符號,會保留譜線的原因是想要盡量在下一個任務中保留盡可能  
多的樂譜資訊,但這樣做可能會增加電腦計算需求和建立模型數據的難度,因  
此移除五線譜還是光學樂譜辨識使用的主流方式。  
圖4: 將五線譜圖像進行黑色像素之水平投影,下圖的最大值的部分為譜線之位置

譜線偵測最簡單的方法是在圖像中黑色像素的水平投影中找到局部的最大  
值15,假設譜線是直的且水平的情況下,這些局部最大值代表了線的  
位置,因此為了排除輸入影像中譜線傾斜的可能性,可以使用不同的圖像旋轉  
角度進行多個水平投影,保留局部最大值較高的圖像,就可得出譜線水平需要  
的角度資訊。偵測譜線可以運用到的方式有很多種,舉兩篇研究為例,其中一  
7

## **PAGE 18**

種為Miyao and Nakano\[16\]使用霍夫變換(Hough transform)來偵測譜線,另一種方  
式是使用垂直掃描線\[17\],這個過程基於線鄰接圖(line adjacency graph, LAG)的  
分析,它可以找尋直線有可能會出現的潛在位置,也就是滿足長寬比、連通性  
和曲率等相關基準的部分。  
移除完樂譜譜線後,接下來就是要進行音樂符號的分割與辨識,分割過程  
包括定位和隔離音樂符號以進行辨識,但由於影像掃描的畫質或是樂譜本身紙  
張的缺陷,導致這個操作的複雜性不僅涉及到譜線本身的扭曲,還有破碎和重  
疊的符號,以及符號尺寸和形狀的差異,甚至是符號數量密集的區域,使得抓  
取單獨的且完整的符號非常困難,而且樂譜上的音樂符號種類也是非常多樣,  
所以分割與辨識音樂符號也一直是光學樂譜辨識研究的重點\[18\]。  
2.4 樂譜重建

樂譜重建的意思是從圖形識別的形狀中提取符號在音樂中代表的語義,並  
將其儲存在音樂數據結構中,基本上這涉及將圖形識別的音樂特徵與五線譜資  
訊結合起來,以正確生成代表掃描圖像含義的音樂數據結構,這是通過解釋五  
線譜中偵測到的各種基礎資訊之間的空間關係來完成的。如果正在處理光學字  
符識別,這是一個簡單的任務,因為需要判斷的資訊主要是一維的,但是在光  
學樂譜識別中,音樂資訊基本上是二維的,垂直方向表示音高,這是因為音高  
是由音符符頭在譜線的高度位置所決定,水平方向表示時間,因此音樂符號位  
置資訊非常重要,同一個圖形形狀在不同情況下可能代表不同的含義,例如要  
確定兩個音符之間的彎曲線是圓滑線還是連結線,就有必要考慮兩個音符的音  
高,此外音樂規則涉及大量符號,在樂譜中它們的功用也相距甚遠。  
8

## **PAGE 19**

2.5 符號格式構建

在重建完樂譜資訊後,光學樂譜辨識的最終階段就是符號格式構建,就是  
要將音樂資訊轉換為音樂數位介面(musical instrument digital interface, MIDI)  
格式,然而多年來已經開發了幾種其他用於音樂的音樂編碼格式,請參見表1。  
大部分的光學樂譜辨識系統是非自適應性的,也就是說它們不會通過使用來提  
升其性能,有些研究試圖通過合併多個光學樂譜辨識系統來克服這一限制\[19\],  
此外光學樂譜辨識系統的結果很多僅能夠用於識別印刷樂譜,除了PhotoScore,  
它可以處理手寫樂譜外,其他大多數系統在輸入圖像中如果有缺陷時會失敗,  
例如影印或品質低劣的文件,因此這也是光學樂譜辨識系統能夠加強的方向。  
表1:相關的OMR軟體和程式\[6\]

| Software and program | Output file |
| :---- | :---- |
| SmartScore | Finale, MIDI, NIFF, PDF |
| SharpEye | MIDI, MusicXML, NIFF |
| PhotoScore | MIDI, MusicXML, NIFF, PhotoScore, WAVE |
| Capella-Scan | Capella, MIDI, MusicXML |
| ScoreMaker | MusicXML |
| Vivaldi Scan | Vivaldi, XML, MIDI |
| Audiveris | MusicXML |
| Gamera | XML files |

9

## **PAGE 20**

3.1 背景知識

第三章 深度學習

傳統的機器學習(machine-learning)技術在處理原始形式的自然數據方面存  
在著局限性,構建一個圖型識別(pattern recognition)或機器學習系統需要嚴謹的  
工程設計和一定程度的領域專業知識,才能夠設計出一個特徵提取器,將原始  
數據(例如圖像的像素值)轉換為適合的內部表示方法或特徵向量,從中學習  
子系統(通常是分類器)可以檢測或分類輸入中的模式,為了簡化這個過程,  
於是機器學習演化出了一類加強特徵學習(representation learning)的技術,稱為  
深度學習(deep learning) \[20\]。。  
深度學習方法是一種具有多層特徵的特徵學習方法,通過組合簡單但非線  
性的模塊獲得每一個層次的特徵,每個模塊都將表示在一個層次轉換為更高層  
次的特徵表示,通過組合足夠多的這樣的轉換,可以學習到非常複雜的函數,  
對於分類任務,較高層的資訊放大了對輸入較重要的部分,並抑制了不相關的  
資料變化,例如一張圖像以像素值的陣列形式呈現,特徵學習中第一層的特徵  
通常表示圖像中特定方向和位置的邊緣的存在或不存在;第二層通常通過識別  
邊緣的特定排列來檢測圖案,而不考慮邊緣位置的微小變化;第三層可能將圖  
案組合成更大的組合,對應於熟悉對象的部分,隨後的層次將檢測對象作為這  
些部分的組合。深度學習的關鍵在於這些特徵層不是由人類工程師設計的,它  
們是使用通用學習程序從數據中學習得來的。  
深度學習在解決多年來一直困擾著人工智慧的問題取得了重大進展,它在  
檢測高維數據中的複雜結構表現出色,因此適用於科學、商業和政治的許多領  
域,除了在影像識別\[21\]和語音識別\[22\]方面打破記錄外,它還在預測潛在藥  
物分子的活性\[23\]、分析粒子加速器數據\[24\]、重建腦回路\[25\],以及預測非編  
碼DNA中突變對基因表達和疾病的影響\[26\]等方面擊敗了其他機器學習技術,  
更令人驚訝的是,深度學習在自然語言理解\[27\]的研究中取得了極具潛力的成  
果,尤其是主題分類、情感分析、問題解答\[28\]和語言翻譯\[29\]。  
10

## **PAGE 21**

3.2 在OMR中的發展

隨著深度學習在越來越多領域取得非常巨大的突破,光學樂譜辨識也開始  
將深度學習運用在各個階段中,在五線譜處理階段中,Gallego Antonio-Javier等  
人\[30\]提出一種自動編碼器的方式。在傳統的圖像處理技術當中,這些方法基  
於手工設計的轉換,問題也可以從深度學習的角度來解決,該方法涉及選擇性  
自動編碼器(selectional auto-encoders),它選擇輸入特徵集的適當特徵,在移除  
五線譜這個問題的背景下,該模型被訓練為選擇給定圖像中屬於音樂符號的像  
素,從而去除譜線,他們的結果表明,所提出的技術相當有競爭力,並在處理  
灰階的輸入圖像時顯著優於其他處理五線譜的策略。  
在偵測音樂符號的階段中,Lukas Tuggener等人\[31\]提出一種叫做深度分水  
嶺偵測器(deep watershed detector, DWD)的符號偵測方式,這個偵測方式基於合  
成能量圖(synthetic energy maps)和分水嶺變換(watershed transform),專門設計用  
於處理包含大量非常小對象的高分辨率圖像,因此能夠處理完整的樂譜頁面,  
研究中呈現了常見音樂符號的最新檢測結果,並展示了深度分水嶺偵測器與合  
成樂譜和手寫音樂有著同樣良好的偵測效果。  
在樂譜重建階段中,準確判斷符號語義非常重要,每個符號的語義是與其  
他符號的關係來定義的,因此光學樂譜辨識系統必須統整各種符號來推斷檢測  
到的對象之間的關係,到目前為止這個階段已經通過制定一組預先定義的規則  
或語法來解決,但這很難很好地通用化。Alexander Pacha等人\[32\]將這個階段定  
義為一個深度學習問題,符號統整被建模為一個圖表,它存儲了符號之間的語  
法關係,就能夠獲取樂譜文檔中符號的配置,這項研究在手寫樂譜符號資料  
庫MUSCIMA++上的各項辨識結果中都有九成以上的高精準度。  
11

## **PAGE 22**

3.3 遷移學習

遷移學習\[33\]是一種機器學習方法,這方法的核心概念為將為了第一個任  
務開發的學習模型被重複利用作為第二個任務中的學習模型的起點\[34\],也就是  
說將解決一個問題時獲得的知識儲存起來,並應用於不同但相關的問題上,基  
本上它是使用一個已經訓練過的神經網絡來實現在學習任務二時能有更短的訓  
練時間(如圖5)。遷移學習是現今多階段神經網絡中的一種突出的學習方式,這  
種神經網絡被稱為深度神經網絡(deep neural network, DNN),也是深度學習的  
一種。  
Task 1  
Data 1 \-\> Model 1 \-\> Head 1 \-\> y1  
Transfer learning

Task 2  
Data 2 \-\> Model 1 \-\> Head 2 \-\> y2  
圖5:遷移學習示意圖

在圖5中如何進行遷移學習,舉例來說就是將原本在Task 1中用 Model 1  
對Data 1進行訓練,而產生Head1利用Data1的特徵去作出辨識,輸出y1的  
辨識結果;Task2則是利用已經創建好的Model 1,輸入Data2進行訓練,產生  
Head 2使用 Data2的特徵進行預測,輸出y2的預測結果。  
12

## **PAGE 23**

3.4 深度學習工具

本論文的開發環境是使用MATLAB R2023a,在深度學習的方面使用系統  
支援的預訓練深度神經網絡進行遷移學習,概述其中使用到的三種圖像分類深  
度神經網絡:GoogLeNet\[35\]、ResNet\[36\]、Inception-v3\[37\]。  
3.4.1 GoogLeNet

GoogLeNet 網絡是由Szegedy\[35\]等人提出,設計時考慮了計算效率和實  
用性,以便可以在僅有中央處理器的電腦上運行,不一定需要圖形處理器協助  
運算,甚至可以在計算資源有限的設備上運行,特別是在電腦記憶體較低的  
情況下。在僅計算有參數的層數時,該網絡深度為22層,如果再加上池化層則  
為27層,用於構建網絡的整體層數約為100層,在分類器之前使用的平均池化層  
是基於文獻\[38\],此研究中還有一個額外的線性層,能夠輕鬆地將網絡適應其他  
標籤集。  
考慮到網絡相對較大的深度,有效地將梯度傳播回所有層是一個重點,  
較淺層網絡在辨識上的良好表現指出,網絡中間層產生的特徵應該非常有  
差異性,通過添加連接到這些中間層的輔助分類器,預期在分類器的較低  
階段中進行區分,這是為了解決梯度消失(vanishing gradient)問題,同時提供  
正規化(regularization)。這些分類器采用較小的卷積網絡形式,所有的卷積操  
作,包括在各個模組內部的卷積,都使用整流線性激活函數(rectified linear unit  
activation function, ReLU),網絡中的輸入端為零均值化的224x224的RGB色彩  
圖像。  
3.4.2 ResNet

ResNet 模型是由Kaiming\[36\]等人提出,是根據簡易網絡(plain network)所  
創建。簡易網絡的基礎流程,主要受到VGG網絡\[39\]原理的啟發。卷積層主要  
使用3×3的濾波器,並遵循兩個簡單的設計原則:(i)對於相同大小的輸出特  
徵圖,每層具有相同數量的濾波器;(ii)如果特徵圖大小減半,則濾波器的數  
量加倍,以維持每層的時間複雜度。再來通過具有步幅(stride)為2的卷積層直  
接進行下採樣,最後以全局平均池化層和具有歸一化指數(softmax)的全連接層  
13

## **PAGE 24**

結束,這樣的設計讓簡易網絡比VGG網絡有更少的濾波器和較低的複雜度。

殘差網絡(residual network, ResNet)基於上述簡易網絡的結構,每兩層插入  
捷徑連接,將簡易網絡轉換為對應的殘差版本。當輸入和輸出具有相同的大小  
時,可以直接使用恆等捷徑(實線)。當輸入和輸出大小增加時(虛線),這時會  
有兩種選擇:(A)捷徑仍然執行恆等映射,並為增加的尺寸填充額外的零條  
目,此選項不引入額外的參數;(B)使用投影捷徑來匹配大小,通過1×1卷  
積層實現。對於這兩種選項,當捷徑通過兩個大小的特徵圖時,它們以步幅為2  
執行。  
3.4.3 Inception-v3

Inception-v3是由Szegedy \[37\]等人提出的一種深度卷積神經網絡架構,  
旨在提高計算機視覺任務的性能,該架構通過使用Inception 模塊,就是具  
有不同尺寸的卷積核(convolutional kernel)之並聯操作,來提高網絡的效率和  
性能。Inception-v3 在網絡中引入了許多創新的設計,像是使用因式分解卷  
積(factorized convolution)、網格減少(grid reduction)技術和全局平均池化(global  
average pooling)等,這些設計讓其在圖像分類、物體檢測和圖像識別等任務上表  
現出色。  
Inception-v3 的架構包括多個 Inception 模塊,每個模塊都具有不同的卷積核  
大小和操作。通過巧妙地組合這些模塊,它能夠在保持計算效率的同時提高準  
確性。此外,Inception-v3還使用了全局平均池化層來幫助提取特徵並減少參數  
量,從而減少過擬合(overfitting)的風險。這個網絡的佈局還將傳統的7x7卷  
積層分解為三個基於相同思想的3×3卷積層,可以節省計算資源,同時保持對  
特徵的有效提取。  
14

## **PAGE 25**

第四章 深度學習與OMR之結合

經過第二章對於光學樂譜辨識的介紹,對於樂譜進行辨識大致分為四個階  
段,也就是影像前置處理、音樂符號辨識、樂譜重建和符號格式構建,而本研  
究是在音樂符號辨識這個階段使用深度學習進行加強,故在第四章會詳細說明  
本研究將光學樂譜辨識與深度學習結合的實際操作方法。  
4.1 系統整體流程

圖6為本研究在將深度學習工具應用至光學樂譜辨識的整體系統流程,  
並且根據圖1中的框架來進行每個步驟的分類:在輸入的樂譜中是使用特製格  
式的數位樂譜圖檔;在影像前置處理中會對輸入的樂譜圖片進行二值化與灰階  
化;在音樂符號辨識中為了要將其之中的音符單音的圖片擷取出來,會把樂譜  
進行一系列的影像處理,再來將音符資料庫的一部分用來訓練深度學習模型,  
製作出音符的分類器;另一部分則是將音符圖片檔案按照目標樂譜的演奏順序  
進行儲存,接著再使用這分類器去辨識這些排序好的音符圖片檔案;在樂譜重  
建中將辨識完的單音圖片轉換成相對應的節拍時間,接著根據音符圖片擷取中  
的資訊取得音符的音高;在符號格式構建中則是將音高與節拍轉換成MIDI的音  
檔格式;最後的輸出就是目標樂譜的一段音樂旋律。  
在說明完本研究各個框架所運用的方法後,在來講解每個方法的詳細過  
程。系統一開始的輸入為鋼琴樂譜的圖檔,將彩色的圖檔轉換成灰階圖像讀取  
至系統,接著對整張樂譜進行二值化。接著在音符圖片擷取中,需要先找出五  
線譜及小節線的線條位置,對整張樂譜分別進行水平和垂直的像素投影,就能  
夠將五線譜及小節線去除掉,但原本五線譜中的音符會出現被去除掉的空隙,  
因此要在將其空隙補回黑色像素。去除完的小節線會保留其位置資訊用來切割  
出每個小節,然後以每個小節為單位偵測音符頭,獲取其中心座標、數量、出  
現順序和音符頭半徑大小等資訊。利用上述獲取的這些資訊分成兩部分處理,  
一個部分是根據每個音在五線譜的相應位置而判斷出來其音高;另一部分是將  
每個音符依照前後距離的不同切割下來相應大小的圖片,並且對此圖片進行消  
除雜訊。  
15

## **PAGE 26**

在擷取單音音符的圖片後由於要分別進行訓練與辨識,選取一部分樂譜製  
作成深度學習模型訓練用的數據檔,並將其輸入至深度神經網絡訓練,製作成  
單音音符的分類器,接著選取要辨識的目標樂譜,依照演奏順序來排列音符圖  
片檔案,用上述的分類器對排列好的的音符圖片檔案進行辨識,就能夠分辨出  
圖片中的音符種類。接著將音符種類轉換成相對應的節拍時間,然後在偵測符  
頭時獲取的位置轉換成音高,最後根據每個音的節拍與音高照順序創建出MIDI  
格式的播放樂譜,再將其輸出成MIDI的音檔,最後本研究之系統就能播出目標  
樂譜的音樂旋律。  
16

## **PAGE 27**

\[圖6: 音符的深度學習模型訓練架構圖\]  
（圖表內容流程：  
樂譜 \-\> 影像前置處理(二值化, 灰階化) \-\> 音樂符號辨識  
音樂符號辨識 \-\> 音符圖片擷取 \-\> 音符資料庫  
音符資料庫 \-\> 深度學習模型 \-\> 更新參數 \-\> 音符分類器  
音符資料庫 \-\> 目標樂譜排序 \-\> 音符分類器  
音樂符號辨識 \-\> 樂譜重建 (判定音高, 判定節拍)  
樂譜重建 \-\> 符號格式構建(MIDI格式) \-\> 音檔輸出  
）  
圖6:音符的深度學習模型訓練架構圖

17

## **PAGE 28**

4.2 影像前置處理

本論文之研究硬體設備的中央處理器為12th Gen Intel(R) Core(TM) i5-  
12500,圖形處理器為NVIDIA GeForce RTX 3080,光學樂譜辨識的開發環境  
為MATLAB R2023a版本,深度學習則是使用MATLAB中的應用程式(deep  
network designer)來完成,製作MIDI音檔則是匯入Java工具協助製作。  
在圖6中的影像前置處理階段中,會對樂譜圖像先進行二值化,也就是  
將原本有RGB三原色的圖檔替換成只有0和1的影像,這會使得同一種顏色  
都是相同的色彩亮度,不會出現像是深紅或淺紅的顏色。接下來由於輸入樂  
譜這種數位圖檔很容易受到脈衝雜訊(impulse noise)的干擾,所以進行光學樂  
譜辨識之前,為了盡可能地減少輸入圖像中的雜訊,使用高斯濾波器(Gaussian  
filter)\[40\]對圖像進行雜訊抑制,然後將鋼琴樂譜進行灰階化,也就是將原本  
RGB三色的圖片轉換成只有黑白的影像,這是因為樂譜本身也不會出現太多的  
顏色種類,直接進行處理不會影響樂譜品質。接著將樂譜圖像寬度按照原圖比  
例縮放至設定的固定大小,以便使樂譜滿足整體系統的需求,也能夠提升後續  
任務的處理效率和方便性。  
18

## **PAGE 29**

\[圖7\]  
(a) 圖：樂譜圖像  
(b) 圖：水平投影直方圖  
圖7:將(a)圖中的樂譜使用像素的水平投影後以直方圖(b)表示,(a)圖中  
的紅線位置就是(b)圖中的波峰位置  
4.3 音樂符號辨識

在這個小節說明圖6中在音樂符號辨識的方框中,如何進行音符圖片擷  
取、目標樂譜排序、深度學習模型與音符分類器等步驟。  
4.3.1 音符圖片擷取

在音符圖片擷取的階段中,得到經過上述方式處理過的樂譜後,找出五  
線譜的位置是整個光學樂譜辨識的核心步驟,也是第一個需要獲取的資訊,  
在樂譜中五線譜所出現的位置基本上橫向黑色的像素是最多的,因此使用一  
種大部分相關研究常用的叫做水平投影(horizontal projection)的方式並進行歸一  
化(normalized),這個概念非常直觀又簡單,就是將一張上個步驟處理過之樂譜  
的水平方向的黑色像素數量除以水平方向的像素總量,得出的數值便是每行像  
素中黑色像素之佔比,也因此只會出現0到1之間的數值,並依此生成一個歸  
一化後之亮度直方圖(intensity histogram),波形的峰值就是五線譜的位置,如  
圖7所示,這步驟也會將這資訊儲存起來以利音樂製作階段能夠確定音高。  
19

## **PAGE 30**

\[圖8\]  
向量(B)=\[121,13,13,14,13,128,13,13,14,13,235\]  
與\[-1,-1,-1,-1,0,1,1,1,1\]進行卷積  
向量(C)=\[-53,-47,-34,-20,-7, 0, \-107,-94,-81,-67, 53\]  
圖8:定位五線譜間中線之方式,圖為將樂譜放大至兩個五線譜來分析,  
虛線為表示對應空白寬度大小  
圖8說明如何將樂譜分割成各自的段落部分,首先對圖7(b)的水平投影  
之直方圖進行二值化,閾值設為其平均值加上兩個標準差,這將得到一個向  
量(A),讓這個直方圖只會出現0或1兩個值,其中連續的1代表五線譜之黑色  
線條,連續的0代表一個白色間隔,接著根據向量(A)之直方圖的空白長度製作  
出向量(B),表示從頁面頂部到底部之白色間隔的寬度。一個段落部分通常定義  
為兩個五線譜(對大多數樂譜來說,一個用於高音譜,一個用於低音譜),每  
個五線譜之間有4個較小的白色間隔,段落之間有一個較大的白色間隔,為了  
找出包含兩個五線譜的段落部分之中心,將向量(B)與\[1,1,1,1,0,-1,-1,-1,-1\]進行  
卷積運算得出向量(C),如果向量(C)的值接近0,則該值的涵義即為五線譜之間  
的白色間隔的中心位置,然後第偶數個中心位置被定義為段落的中心,第基數  
個中心位置被定義為高低音譜的中心,最後就能將樂譜按照段落進行分割,如  
圖9所示。  
20

## **PAGE 31**

\[圖9: 樂譜分割示意圖\]

圖9:實線為高低音譜的中心線,虛線為段落之間的中心線。

21

## **PAGE 32**

(a)  
(b)  
(c)  
圖10: (a)為五線譜段落原圖,(b)為去除掉五線譜後結果,(c)為去除小節線並補回音符空隙

接下來對樂譜進行去除五線譜,利用水平直方圖的波峰位置來將樂譜上的  
那些相應位置的像素變換為0以去除五線譜線,然而直接移除這些五線譜線可  
能會導致音樂符號支離破碎或變形(見圖10(b)),為了解決這個問題,在填補遺  
失資訊的步驟中對於因為去除五線譜而導致的黑色像素之間的每個間隙,將間  
隙上方和下方的黑色像素直接向間隙的中心投射(見圖10(c))。去除小節線的方  
式就跟五線譜相同,小節線的位置則是使用五線譜之間的中線(圖9之實線)幫助  
我求得,這只要依據它所經過之黑色像素位置即是小節線位置,再將小節線的  
位置儲存起來進行去除與後續分割小節所使用。  
22

## **PAGE 33**

\[圖11: 在每個小節執行霍夫變換抓取音符頭\]  
\[圖12: 對每個音進行方框切割\]  
分割完每個段落後,為了區分高低音譜,將每個段落的兩組五線譜上切割  
開,最後根據小節線的位置將每段進行切割,要以每個小節為基礎來進行音符  
的偵測,這是為了要製作音檔而採取的作法。接著為了正確抓取到音符,能夠  
估算樂譜中每個音符頭的平均半徑是對偵測音符很有幫助的,同一五線譜中兩  
條線之間的距離通常是一個音符頭的高度,使用這個值的一半,也就是音符頭  
的半徑作為初始參數,以小節為單位對所有樂譜圖像執行霍夫變換來去抓取圓  
形的物體(見圖11),然後將所有檢測到的圓的半徑取平均值,以獲得對偵測每  
個音符頭的更好的靈敏度,減少抓取到其他符號的機會。抓取到圖11所標示的  
綠色圓圈後,根據它的圓心座標和圓圈半徑來對每個音進行一個方框之切割,  
方框高度的大小就直接等同於每小節高度,方框寬度則是與前後音符頭的距離  
有關,將所有相鄰符頭距離的一半進行加總並平均設為方框的閾值,如果距離  
太遠則會直接等同於這個閾值。在圖12中抓取的第一個方框就是因為超過此閥  
值,因此其寬度等同於此閾值,而後續的幾個音則是要盡可能框出最大範圍但  
又不框到鄰近的音符,故抓取範圍也會重疊。  
圖11:在每個小節執行霍夫變換抓取音符頭  
圖12:對每個音進行方框切割...  
23

## **PAGE 34**

\[圖13: 找出A、B、C三部分並計算黑色像素數量,B為最大區域故保留\]  
(圖片顯示一個音符被切割後有雜點 A 和線段 C，主體 B 被保留)  
圖13: 找出A、B、C三部分並計算黑色像素數量,B為最大區域故保留

4.3.2 深度學習模型與音符分類器

在辨識音符種類之前,必須要先製作音符的分類器,本論文在這步驟是使  
用深度學習來幫助我進行音符的分類,由於從頭開始製作深度神經網絡需要龐  
大的訓練量來驗證準確性,於是使用三個預先訓練好的圖像分類之深度神經網  
絡,分別是GoogLeNet\[35\]、ResNet\[36\] 和 Inception-v3\[37\]。這些網絡都已接受  
超過一百萬張圖像之訓練,最大可以將圖片分類至1000種類別,像是各種動植  
物或生活上會出現或使用的各種物體都能拿來進行訓練。  
將前面步驟處理過並切割下來的音符輸入網絡進行遷移學習之前,必須符  
合網絡規定的輸入圖像格式,這些網絡都只能使用RGB形式且大小相同的圖片  
進行訓練,於是就將前面步驟每一個所框出範圍之影像切割並轉成彩色形式,  
再將其放置在相同大小之白色背景,根據原音符比例縮放至背景最大高度或寬  
度,這樣就能夠滿足網絡輸入的限制,圖像存至資料夾前還需進行去雜訊處理  
以提升辨識效果,雜訊處理的方式就是先偵測出黑色像素相連之區域,計算每  
個區域的黑色像素數量後,只留下最大的黑色像素區域(見圖13)。  
24

## **PAGE 35**

\[圖14: 深度學習模型的訓練集直方圖\]  
\[圖15: 深度學習模型的驗證集直方圖\]  
製作音符分類器之前,要先將各個相對應種類的音符創建資料夾來對  
深度神經網絡進行訓練,本論文是將偵測到的音樂符號分為七類,有全  
音符(whole)、二分音符(half)、四分音符(quarter)、八分音符(eight)、十六分音  
符(sixteenth)、三十二分音符(thirtytwo)和其他(other),並且將資料分為訓練集(圖  
14)和驗證集(圖15)。  
圖14:深度學習模型的訓練集  
圖15:深度學習模型的驗證集  
由圖14和圖15可以得知訓練集有305張音符圖片,而驗證集有133張圖  
片,故訓練集與驗證集的比例大約為7:3。14中可以看出在八分音符和十六分  
音符的訓練數量較多,這是因為此二類較常在樂譜中出現且此兩類的特徵較相  
25

## **PAGE 36**

\[圖16: 深度神經網絡訓練過程曲線圖\]

似,兩音符之間容易互相辨識,需要更多的訓練素材提高精度。其他音符則是  
因為在樂譜上的出現次數偏少且特徵明顯,蒐集到音符素材的機會就減少許  
多,而驗證集中每個音符種類之間的比例也與訓練集相同。  
在訓練深度神經網絡的過程中(見圖16),可以看到訓練集的辨識率變化,  
也就是使用訓練集自身的檔案來分類音符的成功率,接著再使用訓練中沒使用  
到的檔案進行驗證,這就是驗證集的功能,為了要避免過擬合(overfitting)的情  
況產生,可以隨時看出分類結果會不會過度匹配訓練資料,也就是訓練集的辨  
識率高但驗證集的辨識率效果不好,不過從圖16可以看出訓練集(training)的辨  
識率變化與驗證集(validation)的辨識率變化都非常接近,故判斷本研究訓練好的  
音符分類器沒有產生過擬合的問題。  
圖16:深度神經網絡訓練過程

圖16顯示此深度神經網絡在20次訓練週期中的辨識成功率的變化,每一  
個週期有19次迭代(iteration),訓練完總共有380次,並且辨識率已逐漸趨緩。  
在圖中的淺實線為網絡在訓練時辨識率的變化,實線為將淺實線平滑過後的結  
果,黑點則代表網絡當下對訓練結果進行驗證時辨識率的變化,虛線則是將黑  
點連接起來與訓練結果進行比較。  
26

## **PAGE 37**

\[圖17: 音符圖片排序\]  
(a) H1\_1, H1\_2...  
(b) L1\_1, L1\_2...  
4.3.3 目標樂譜排序

在目標樂譜排序的階段中,就是將樂譜中擷取出的單音音符按照演奏的  
順序進行排序,實際做法先將音符圖片分成高低音譜兩個資料夾,接著根據樂  
譜由左至右的演奏方式來判別音符圖片的先後順序,再將每個圖片的檔名編號  
幫助排序(圖17),這樣音符分類器便能按照排列順序對兩個資料夾的音符進  
行辨識,辨識結果就能夠輸出成相對應的英文,也就是全音符(whole)、二分音  
符(half)、四分音符(quarter)、八分音符(eight)、十六分音符(sixteenth)、三十二分  
音符(thirtytwo)和其他(other)。  
圖17:音符圖片排序,(a)為高音譜圖片資料夾;(b)為低音譜圖片資料  
夾。  
27

## **PAGE 38**

\[圖18: A的音高判斷方式\]  
(圖示說明：五線譜中線、音高判定線與音符A的關係)  
4.4 樂譜重建

在樂譜重建的階段中,需要將音符分類器的輸出結果轉換成相對應的節  
拍時間,由於樂譜的BPM(beats per minute)通常為120,代表一分鐘需要彈120個  
四分音符,因此預設四分音符為0.5秒,至於其他音符的時間則根據與四分音符  
的比例來取得,也就是全音符為2秒、二分音符為1秒、八分音符為0.25秒、十  
六分音符為0.125秒和三十二分音符為0.0625秒,至於其他這個分類就代表不需  
要播放,時間為0秒,這樣就能獲取每個音符播放的起始時間和持續時間,以  
利MIDI音檔的製作。  
在得到每個音符的時間之後,還需要每個音符的音高資訊,這個資訊可以  
藉由前面音符步驟求出的音符頭的中心資訊來判斷。這就需要得知中心位置的y  
座標在原本五線譜之間的相對關係,做法為先根據五線譜線之間的寬度來設定  
判定基準,也就是每四分之一個五線譜之寬度會有一條基準線來判斷音高,至  
於五線譜寬度則會將所處部分的五線譜間隙加總後進行平均,超出五線譜部分  
也藉這個寬度數值去進行添加。  
圖18:A的音高判斷方式

圖18中的粗實線為五線譜,細實線為五線譜中線,虛線為細實線和上下兩  
條粗實線之中線,這兩條虛線就是A音符的音高判定線,因為A音符出現在虛  
線範圍內,故每兩條虛線涵蓋的高度範圍決定一個音。  
28

## **PAGE 39**

4.5 符號格式構建

經過以上步驟後製作音檔所需之資訊皆已獲得,不過要將現有的資訊轉  
變一下表現形式讓其可以滿足MIDI的製作格式,MIDI音檔需要一個音的七  
個資訊,分別是節拍表示的起始時間、節拍表示的持續時間、播放聲道、MIDI  
音高、音量、以秒表示的起始時間和以秒表示的持續時間。第一項的起始時間  
是根據第二項的持續時間所求得,每個音的起始時間就是前一個音以前所有音  
的持續時間之總和,持續時間則就是辨識音符步驟取得的節拍時間,播放聲道  
則是用五線譜處理階段就使用的高低音譜分開處理之方式來區分,音高就直接  
輸入進去上述步驟取得的資訊,音量則是將每個音設定為固定大小,而第一項  
的數值與第六項相同,第二項與第七項相同,這樣便能製作出樂譜中表示的音  
樂,詳情見表2。  
表2:創建 MIDI 樂譜格式

| 起始時間(節拍) | 持續時間(節拍) | 聲道 | 音高 | 音量 | 起始時間(秒) | 持續時間(秒) |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 0 | 0 | 1 | 60 | 40 | 0 | 0 |
| 0 | 0 | 2 | 53 | 40 | 0 | 0 |
| 0 | 0.34 |  | 81 | 40 | 0 | 0.34 |
| 0 | 0.65 | 2 | 41 | 40 | 0 | 0.65 |
| 0.34 | 0.34 | 1 | 69 | 40 | 0.34 | 0.34 |
| 0.65 | 0.65 | 2 | 48 | 40 | 0.65 | 0.65 |
| 0.69 | 0.34 | 1 | 76 | 40 | 0.69 | 0.34 |
| 1.04 | 0.34 | 1 | 69 | 40 | 1.04 | 0.34 |
| 1.30 | 0.65 | 2 | 53 | 40 | 1.30 | 0.65 |
| 1.39 | 0.34 | 1 | 81 | 40 | 1.39 | 0.34 |

表2是節錄偵測到的樂譜的前十個音,用表中的第十個音來說明的話,也  
就是這個音在這段音樂的1.39秒的時候播放;持續了0.34秒的時間;並且是使  
用聲道1來播放,代表這是高音譜上的音符;此音符還有著81的MIDI音高,  
這是MIDI 內建的音高編號(見圖19);此音符的音量設定為40,這是每個音都  
相同的。  
29

## **PAGE 40**

\[圖19: 五線譜對應位置之音符音名與其對應的MIDI音高\]

4.6 深度學習評估方式

由於本論文之深度學習辨識種類有七種,也因此會先製作出深度學習模型  
辨識出的音符之混淆矩陣(confusion matrix) 41,其中根據這個矩陣所  
去計算的各種評估模型的參數\[42\],參數詳細的公式如式(1)(2)。在圖20所示,  
假設現在要針對A來計算,true positive (TP)代表實際辨識音符和辨識音符結果  
相同; false positive (FP)表示實際值是錯誤的,在圖中它是B和C,但模型辨識  
它是A,所以是除了TP值之外的對應列的值的相加;true negative(TN)表示非  
目標之實際值和預測值的意思相同,也就是對於A而言B和C是負面分類。它  
是所有非A行和列的值相加;false negative (FN)表示實際值在圖中是它是A,但  
模型辨識它是B和C,所以透過除TP值之外的相鄰行的加總來計算的。  
準確率(accuracy)就是代表整體音符辨識的正確性,在多目標分類中就是分類正  
確的音符圖片總數除以目標樂譜的音符總數。  
圖19:五線譜對應位置之音符音名與其對應的MIDI音高

30

## **PAGE 41**

\[圖20: 參數在多目標混淆矩陣之位置\]

圖20:參數在多目標混淆矩陣之位置,圓圈表示當前計算種類

精確性(precision)為實際正確預測音符數量除以總辨識音符之數量,而召回率  
(recall)為實際辨識音符正確,由於這兩個參數皆須個別對每個類別進行計  
算,以圖20為例,就是以A為視角來進行計算,以下公式所需的數值會出現在  
混淆矩陣的位置,故還需以B和C的視角對其計算精確性及召回率,最後這兩  
個數值還會根據每個音符的數量比例進行加權平均。  
$Precision=\\frac{TP}{TP+FP}$ (1)

$Recall=\\frac{TP}{TP+FN}$ (2)

31

## **PAGE 42**

第五章 辨識音符結果與比較

5.1 使用 GoogLeNet 辨識結果

圖21為使用深度學習模型 GoogLeNet 辨識輸入樂譜的音符種類的輸出並  
繪製成混淆矩陣,顏色越深表示次數越高,而在對角線上的數值越高,表示模  
型在這個類別上的準確度越高,透過顏色深淺可以明顯地看出模型在識別十六  
分音符和八分音符上的性能相較其他音符來說非常好,分別有158和117次正  
確識別,藉由表3統計圖中的相關評估參數,也能從這兩個音符的辨識成功率  
驗證此說法。  
\[圖21: GoogLeNet的辨識結果在混淆矩陣中的展示\]

圖21: GoogLeNet的辨識結果在混淆矩陣中的展示

表3: GoogLeNet的整體準確率和各分類的精確性和召回率及加權平均

|  | eighth | half | other | quarter | sixteenth | thirtytwo | 加權平均 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 精確性 | 97.5% | 100% | 100% | 91.67% | 98.14% | 81.82% | 96.93% |
| 召回率 | 99.15% | 100% | 88.89% | 84.62% | 98.14% | 90% | 97.2% |
| 整體準確率 |  |  |  |  |  |  | 97.2% |

32

## **PAGE 43**

5.2 使用ResNet 辨識結果

圖22為使用深度學習模型 ResNet 辨識輸入樂譜的音符種類的輸出並繪製  
成混淆矩陣,顏色越深表示次數越高,而在對角線上的數值越高,表示模型在  
這個類別上的準確度越高,透過顏色深淺可以明顯地看出模型在識別十六分音  
符和八分音符上的性能相較其他音符來說非常好,分別有159和101次正確識  
別,藉由表4統計圖中的相關評估參數,也能從這兩個音符的辨識成功率驗證  
此說法。  
\[圖22: ResNet的辨識結果在混淆矩陣中的展示\]

圖22: ResNet的辨識結果在混淆矩陣中的展示

表4: ResNet的整體準確率和各分類的精確性和召回率

|  | eighth | half | other | quarter | sixteenth | thirtytwo | 加權平均 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 精確性 | 100% | 100% | 88.89% | 48.15% | 94.62% | 85.71% | 94.44% |
| 召回率 | 87.41% | 100% | 88.89% | 100% | 98.76% | 60% | 92.21% |
| 整體準確率 |  |  |  |  |  |  | 92.21% |

33

## **PAGE 44**

5.3 使用 Inception-v3 辨識結果

圖23為使用深度學習模型 Inception-v3 辨識輸入樂譜的音符種類的輸出並  
繪製成混淆矩陣,顏色越深表示次數越高,而在對角線上的數值越高,表示模  
型在這個類別上的準確度越高,透過顏色深淺可以明顯地看出模型在識別十六  
分音符和八分音符上的性能相較其他音符來說非常好,分別有152和118次正  
確識別,藉由表5統計圖中的相關評估參數,也能從這兩個音符的辨識成功率  
驗證此說法。  
\[圖23: Inception-v3的辨識結果在混淆矩陣中的展示\]

圖23: Inception-v3的辨識結果在混淆矩陣中的展示

表5: Inception-v3的整體準確率和各分類的精確性和召回率

|  | eighth | half | other | quarter | sixteenth | thirtytwo | 加權平均 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 精確性 | 96.72% | 100% | 100% | 92.31% | 98.7% | 60% | 96.59% |
| 召回率 | 100% | 100% | 88.89% | 92.31% | 94.41% | 90% | 95.95% |
| 整體準確率 |  |  |  |  |  |  | 95.95% |

34

## **PAGE 45**

5.4 模型辨識結果比對與分析

樂譜在經過本研究之光學樂譜辨識的方式,將其經過二值化、像素投影、  
去除五線譜及小節線、填補音符被去除掉的空隙和切割出單音等一系列前置處  
理後,透過三種不同模型 GoogLeNet、ResNet、Inception-v3 在同樣的音符訓  
練資料集和訓練設定參數下,遷移學習至辨識音符圖片的情況下。在表6可以  
清楚地看出所有模型在每種辨識類型下的辨識效果。  
表6:統整三種模型的整體準確率和各分類的精確性和召回率

| GoogLeNet | eighth | half | other | quarter | sixteenth | thirtytwo | 加權平均 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 精確性 | 97.5% | 100% | 100% | 91.67% | 98.14% | 81.82% | 96.93% |
| 召回率 | 99.15% | 100% | 88.89% | 84.62% | 98.14% | 90% | 97.2% |
| 整體準確率 |  |  |  |  |  |  | 97.2% |
| **ResNet** | **eighth** | **half** | **other** | **quarter** | **sixteenth** | **thirtytwo** | **加權平均** |
| 精確性 | 100% | 100% | 88.89% | 48.15% | 94.62% | 85.71% | 94.44% |
| 召回率 | 87.41% | 100% | 88.89% | 100% | 98.76% | 60% | 92.21% |
| 整體準確率 |  |  |  |  |  |  | 92.21% |
| **Inception-v3** | **eighth** | **half** | **other** | **quarter** | **sixteenth** | **thirtytwo** | **加權平均** |
| 精確性 | 96.72% | 100% | 100% | 92.31% | 98.7% | 60% | 96.59% |
| 召回率 | 100% | 100% | 88.89% | 92.31% | 94.41% | 90% | 95.95% |
| 整體準確率 |  |  |  |  |  |  | 95.95% |

在表6中可以看出每個模型在二分音符的辨識率都有100%,但由於在樂  
譜實際上出現的次數過少,故不會將其認定為辨識效果最好的音符,而是會  
綜合比較音符的辨識成功率和出現次數來推論。GoogLeNet 模型的整體準確率  
有達到97.2%,加權平均後的精確性和召回率有96.93%和97.2%,由於出現次  
數最多以及在辨識成功率差不多的情況下,在辨識十六分音符的情況下效果  
最好,而因為成功率較低且出現次數差不多,辨識三十二分音符時則表現最  
差; ResNet 模型的整體準確率有達到92.21%,加權平均後的精確性和召回率有  
94.44%和92.21%,由於出現次數最多以及在辨識成功率差不多的情況下,在  
辨識十六分音符的情況下效果最好,而因為成功率較低且出現次數差不多,辨  
識四分音符和三十二分音符時則表現最差; Inception-v3 模型的整體準確率有達  
35

## **PAGE 46**

到95.95%,加權平均後的精確性和召回率有96.59%和95.95%,由於出現次  
數最多以及在辨識成功率差不多的情況下,在辨識十六分音符的情況下效果最  
好,而因為成功率較低且出現次數差不多,辨識三十二分音符時則表現最差。  
三種模型在整體的辨識表現均皆有達到90%以上的水準,而其中  
GoogLeNet 的辨識效果最好,Inception-v3次之,ResNet 最差,這跟MAT-  
LAB 網站\[43\]統整的準確度比較(GoogLeNet 最好,ResNet次之,Inception-v3 最  
差)有所差異,原因有可能是訓練資料與深度神經網絡的相容性所影響到結果的  
不同。至於在這三種網絡中辨識效果最好與最壞的種類都會出現相同的音符,  
這樣的結果很有可能跟樂譜上音樂的表現方式有關,因為在大部分耳熟能詳的  
流行樂或古典樂的鋼琴樂譜中,有一些特定音符會頻繁的一直被使用(如八分音  
符或十六分音符),或是會有特定的幾種音符極少被使用(如二分音符或三十二分  
音符),所以會造成如果辨識成功或失敗時產生的指標出現極端值,所以才會再  
計算一次加權平均以消除極端值的影響,來正確評估模型的辨識效果。  
36

## **PAGE 47**

5.5 本論文與相關文獻之OMR系統比較

在本論文的實作部分中,光學樂譜辨識的部分階段是參考Andy Zeng\[1\]等  
人提出的文獻來製作,故表7為本論文與 Andy Zeng 等人所提出之文獻之實作  
方法的統整,表示本輪文在那些階段參考他們所提出的研究方法,以及那些階  
段使用與他們相異的手段處理問題,還有與他們的系統之間的表現在何處優  
劣。  
在輸入樂譜圖像的部分中,本論文和文獻\[1\]皆須以特製規格的數位樂譜  
進行光學樂譜辨識,但本論文在輸入樂譜方面的限制較為彈性,但會降低後續  
的辨識效果,而文獻\[1\]無法將規格不符的樂譜進行處理;在前處理樂譜影像的  
階段中,本論文與文獻\[1\]皆先使用高斯濾波器進行圖像的消除雜訊,再進行樂  
譜的二值化與灰階化,有助於更好的辨識音符;在五線譜與小節線的去除階段  
中,本論文和文獻\[1\]皆是使用像素投影的方式定位再去清除線條;在偵測音符  
頭的方式中,雖然本論文與文獻\[1\]皆是使用霍夫變換求圓,但本論文有進行參  
數的調整以有更好的抓取效果。  
接下來皆是相異的研究方式,在切割圖像以進行偵測音符頭的階段中,  
本論文是採取小節的大小來進行切割,並且再將高低音譜分別進行處理,文  
獻\[1\]則是採取每個段落(包含數個小節)且不分開高低音譜進行處理;在音符辨  
識種類中,本論文是將分類數量分為七種,文獻\[1\]則只分為三類;在辨識音符  
的階段中,本論文是使用深度學習幫助辨識,且分別測試三種不同預訓練神  
經網絡的效果,文獻\[1\]則是利用樂譜中各種符號的位置關係來去判斷音符種  
類;在辨識音符後輸出成節拍的準確率中,本論文的平均成功率為95.12%,遠  
高於文獻\[1\]的49.67%;但在MIDI音檔的品質中,本論文的旋律表現略差於文  
獻\[1\],這是因為文獻\[1\]在辨識音符後,有對每個音的節拍經過修正,連正確的  
節拍都有經過調整,雖降低準確率,不過讓整體旋律聽感較為悅耳。  
37

## **PAGE 48**

表7:本論文與參考文獻\[1\]的OMR實作方法與結果之比較

| OMR 系統 | 本論文 | 文獻\[1\] |
| :---- | :---- | :---- |
| 輸入圖像 | 特製數位樂譜 | 特製數位樂譜 |
| 前處理樂譜影像 | 高斯濾波器降躁 二值化 灰階化 | 高斯濾波器降躁 二值化 灰階化 |
| 五線譜(小節線)去除 | 水平(垂直)像素投影 | 水平(垂直)像素投影 |
| 偵測音符頭方式 | 霍夫變換求圓 | 霍夫變換求圓 |
| 切割圖像方式 | 依照每個小節區分 高低音譜分開 | 依照每段落區分 高低音譜未分開 |
| 音符辨識種類 | 七種 | 三種 |
| 辨識音符方式 | 深度神經網絡: GoogLeNet ResNet Inception-v3 | 樂譜邏輯運算 |
| 音符分類準確率 | 95.12% | 49.67% |
| 判斷音高方式 | 樂譜空間判斷 | 樂譜空間判斷 |
| MIDI音檔品質 | 較無旋律 | 旋律較完整 |

38

## **PAGE 49**

6.1 結論

第六章 結論及未來展望

本論文在光學樂譜辨識中提出一個結合深度學習之新的辨識方式,可以利  
用自己手邊的樂譜圖片輸入進去系統,並轉換成音樂播放出來。主要為了提升  
音符的辨識效果,在光學樂譜辨識方面就先對樂譜圖像進行了一些處理,先是  
為了降低圖像雜訊使用高斯濾波器,還有去除掉對於辨識影像有極大干擾的五  
線譜與小節線,去除完線條還要將音樂符號被刪去的線填補回來,才能保證符  
號的特徵不會遺失,在切割單個音符下來辨識時也要將其他一起切進去的較小  
像素之區域去除,做完這些步驟再輸入至深度神經網絡訓練有助於提升辨識精  
度。  
在深度學習的辨識方面,也能透過音符資料庫的種類比例提升準  
確率,就是容易分辨錯誤的音符需要多一些資料給深度神經網絡學習  
特徵,最後將本論文將這些方式結合起來,使用三種深度神經網絡模  
型(GoogLeNet、ResNet、Inception-v3)製成的音符分類器之整體準確率分別為  
97.2%、92.21%和95.95%,可以驗證出本論文在光學樂譜辨識和深度學習的結合  
有不錯的辨識表現,可以幫助音樂的初學者能夠更簡單地將樂譜轉換成音樂。  
6.2 未來展望  
本論文著重於提升辨識音符的成功率,雖然能透過結合深度學習獲得不錯  
的辨識率,但由於使用的深度神經網絡的資料庫輸入限制,需要使用固定大小  
的圖片進行訓練,無法在光學樂譜辨識的階段節省更多的步驟,還有一些未來  
可以加強的部分是偵測音符的方式,由於本論文使用的方法是基於霍夫變換去  
抓取圓形物體,所以圓形以外的符號大部分無法被偵測到,尤其是休止符的部  
分,若能結合更好的偵測音樂符號的方式,也能提高整個系統辨識樂譜的通用  
性和輸出成MIDI音檔的品質,讓一般人能用此系統更快速地認識和了解樂譜相  
關知識,以及更正確地聽到樂譜所代表的音樂。  
39

## **PAGE 50 (參考文獻)**

參考文獻

\[1\] A. Zeng, "Optical Music Recognition CS 194-26 Final Project Report , 2014\.

\[2\] H.K. Yuen et al., "Comparative study of Hough Transform methods for circle  
finding", Image and Vision Computing, UK, pp. 71-77, February, 1990\.  
\[3\] A. Pacha, Dissertation \- Self-learning Optical Music Recognition, TU Wien, July,  
2019, DOI:10.13140/RG.2.2.18467.40484.  
\[4\] A. Swift, "An introduction to MIDI. Available", https://ssur.cc/umfHqDM.

\[5\] H.F. Schantz, The history of OCR, optical character recognition., \[Manch-  
ester Center, Vt.\]: Recognition Technologies Users Association, 1982, ISBN-  
10: 0943072018, ISBN-13: 9780943072012\.  
\[6\] A. Rebelo et al., "Optical music recognition: state-of-the-art and open issues.",  
International Journal of Multimedia Information Retrieval, pp. 173-190., March,  
2012\.  
\[7\] R. Goecke, Building a system for writer identification on handwritten music scores.,  
UNSW Canberra, June, 2003\.  
\[8\] J. Cardoso et al., "Staff Detection with Stable Paths.", IEEE Transactions on Pattern  
Analysis and Machine Intelligence, pp. 1134-1139, July, 2009\.  
\[9\] S.E. George, Visual Perception of Music Notation: On-line and Off-line Recogni-  
tion., IRM Press, July, 2005, ISBN-10:1591402980, ISBN-13:9781591402985.  
\[10\] A. Fornés, J. Lladós, and G. Sánchez, "Primitive Segmentation in Old Handwritten  
Music Scores.", Graphics Recognition. Ten Years Review and Future Perspectives.,  
Berlin, pp. 279-290, 2006\.  
\[11\] C. Dalitz et al., "A Comparative Study of Staff Removal Algorithms.", IEEE  
Transactions on Pattern Analysis and Machine Intelligence, p. 753-766, March,  
2008, DOI: 10.1109/ΤΡΑMI.2007.70749.  
40

## **PAGE 51**

\[12\] J. Cardoso and A. Rebelo, "Robust Staffline Thickness and Distance Estimation  
in Binary and Gray-Level Music Scores.", 2010 20th International Conference on  
Pattern Recognition, pp. 1856-1859, October, 2010, DOI: 10.1109/ICPR.2010.458.  
\[13\] T. Pinto et al., "Music Score Binarization Based on Domain Knowledge.", Pattern  
Recognition and Image Analysis, pp. 700-708, June, 2011, DOI:10.1007/978-3-  
642-21257-487.  
\[14\] P. Bellini, I. Bruno, and P. Nesi, "Optical music sheet segmentation.", Proceedings  
First International Conference on WEB Delivering of Music., Florence, Italy, pp.  
183-190, November, 2001\. DOI: 10.1109/WDM.2001.990175.  
\[15\] R. Randriamahefa et al., "Printed music recognition.", Proceedings of 2nd Inter-  
national Conference on Document Analysis and Recognition, Tsukuba, Japan, pp.  
898-901, October, 1993\. DOI: 10.1109/ICDAR.1993.395592.  
\[16\] H. Miyao, and Y. Nakano, "Note symbol extraction for printed piano scores using  
neural networks.", IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS,  
pp. 548-554, May, 1996\.  
\[17\] N.P. Carter, and R.A. Bacon, Automatic Recognition of Printed Music, in Structured  
Document Image Analysis, H.S. Baird, H. Bunke, and K. Yamamoto, Editors, pp.  
456-465, 1992\.  
\[18\] S. Choudhury et al., "Optical Music Recognition System within a Large-Scale  
Digitization Project.", Online presentation ISMIR 2000 Conference, October, 2000\.  
\[19\] D. Byrd and M. Schindele, "Prospects for Improving OMR with Multiple Rec-  
ognizers.", 7th International Conference on Music Information Retrieval, Victoria,  
Canada, pp.41-46, October, 2006\.  
\[20\] Y. Lecun, Y. Bengio, and G. Hinton, "Deep learning.", Nature 521, pp. 436-444,  
May, 2015, https://doi.org/10.1038/nature14539.  
41

## **PAGE 52**

\[21\] A. Krizhevsky, I. Sutskever, and G.E. Hinton, "ImageNet classification with deep  
convolutional neural networks.", Communications of the ACM, pp. 84-90, May,  
2017\.  
\[22\] T. Mikolov et al., "Strategies for training large scale neural network lan-  
guage models.", 2011 IEEE Workshop on Automatic Speech Recognition and  
Understanding., Waikoloa, HI, USA, pp. 196-201, December, 2011, DOI:  
10.1109/ASRU.2011.6163930.  
\[23\] J. Ma et al., "Deep Neural Nets as a Method for Quantitative Structure-Activity  
Relationships.", Journal of Chemical Information and Modeling, American, pp.  
263-274, January, 2015\.  
\[24\] T. Ciodaro et al., "Online particle detection with Neural Networks based on topo-  
logical calorimetry information.", Proceedings, 14th International Workshop on  
Advanced Computing and Analysis Techniques in Physics Research, Uxbridge, UK,  
pp. 12-30, September, 2011, DOI: 10.1088/1742-6596/368/1/012030.  
\[25\] M. Helmstaedter et al., "Connectomic reconstruction of the inner plexiform  
layer in the mouse retina.", Nature 500, pp. 168-174, August, 2013, DOI:  
https://doi.org/10.1038/nature12346.  
\[26\] M.K. Leung et al., "Deep learning of the tissue-regulated splicing code.", Bioin-  
formatics, p. i121-1129, June, 2014, DOI:10.1093/bioinformatics/btu277.  
\[27\] R. Collobert et al., "Natural Language Processing (Almost) from Scratch.", Journal  
of Machine Learning Research, America, pp. 2493-2537, May, 2011\.  
\[28\] A. Bordes, S. Chopra, and J. Weston, "Question Answering with Subgraph Embed-  
dings.", Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-  
guage Processing, Doha, Qatar, pp. 615-620, October, 2014, DOI: 10.3115/v1/D14-  
1067\.  
42

## **PAGE 53**

\[29\] S. Jean et al., "On Using Very Large Target Vocabulary for Neural Machine Trans-  
lation.", Proceedings of the 53rd Annual Meeting of the Association for Computa-  
tional Linguistics and the 7th International Joint Conference on Natural Language  
Processing, Beijing, China, pp. 1-10, July, 2014, DOI:10.3115/v1/P15-1001.  
\[30\] A.-J. Gallego and J. Calvo-Zaragoza, "Staff-line removal with selectional  
auto-encoders.", Expert Systems with Applications, pp. 138-148, July, 2017,  
DOI:10.1016/j.eswa.2017.07.002.  
\[31\] L. Tuggener et al., "Deep Watershed Detector For Music Object Recogni-  
tion.", 19th ISMIR Conference, Paris, France, pp. 271-279, May, 2018,  
DOI:10.13140/RG.2.2.30510.20807.  
\[32\] A. Pacha J. Calvo-Zaragoza and J. Hajic. "Learning Notation Graph Construction  
for Full-Pipeline Optical Music Recognition.", 20th International Society for Music  
Information Retrieval Conference, Delft, Netherlands, pp. 75-82, November, 2019\.  
\[33\] S. Bozinovski, "Reminder of the First Paper on Transfer Learning in Neu-  
ral Networks, 1976.", Informatica 44, pp. 291-302, September, 2020,  
DOI:10.31449/inf.v44i3.2828.  
\[34\] C. Tan et al. "A Survey on Deep Transfer Learning.", Artificial Neural Networks  
and Machine Learning \- ICANN 2018., pp. 270-279, September, 2018\.  
\[35\] C. Szegedy et al. "Going deeper with convolutions.", 2015 IEEE Conference on  
Computer Vision and Pattern Recognition(CVPR), Boston, MA, USA, pp. 1-9, 2015,  
DOI:10.1109/CVPR.2015.7298594.  
\[36\] K. He et al., "Deep Residual Learning for Image Recognition.", 016 IEEE Confer-  
ence on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA,  
pp.770-778, June, 2016, DOI: 10.1109/CVPR.2016.90.  
\[37\] C. Szegedy, et al., "Rethinking the Inception Architecture for Computer Vision.",  
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las  
Vegas, NV, USA, pp. 2818-2826, June, 2016, DOI: 10.1109/CVPR.2016.308.  
43

## **PAGE 54**

\[38\] M. Lin, Q. Chen, and S. Yan, "Network In Network.", International Conference  
on Learning Representations 2014, pp. 1-10, December, 2013\.  
\[39\] K. Simonyan, and A. Zisserman, "Very Deep Convolutional Networks for Large-  
Scale Image Recognition.", International Conference on Learning Representations  
(ICLR), May, 2015\.  
\[40\] R.A. Haddad and A.N. Akansu, "A class of fast Gaussian binomial filters for  
speech and image processing." IEEE Transactions on Signal Processing, pp. 723-  
727, March, 1991, DOI: 10.1109/78.80892.  
\[41\] S. Stehman, "Selecting and interpreting measures of thematic classifica-  
tion accuracy.", Remote Sensing of Environment, pp. 77-89, October, 1997,  
DOI:10.1016/S0034-4257(97)00083-7.  
\[42\] D. Powers, "Evaluation: From Precision, Recall and F-Factor to ROC, Informed-  
ness, Markedness and Correlation.", Journal of Machine Learning Technologies,  
pp. 37-63, 2008\.

\[43\] Pretrained

Deep

Neural

Networks.

Available

from:https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-

neural-networks.html.

44